<!DOCTYPE html>
<html lang="en">

<head>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-71156606-1');
    </script>
    <meta charset="utf-8" />
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Advancing AI for Humanity</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="./assets/css/main.css" />

    <script type="text/javascript">document.documentElement.className = 'js';</script>
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:image:width" content="3200" />
    <meta property="og:image:height" content="1800" />

    <meta name="generator" content="Ghost 5.12" />
    <style id="gh-members-styles">
        .gh-post-upgrade-cta-content,
        .gh-post-upgrade-cta {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            text-align: center;
            width: 100%;
            color: #ffffff;
            font-size: 16px;
        }

        .gh-post-upgrade-cta-content {
            border-radius: 8px;
            padding: 40px 4vw;
        }

        .gh-post-upgrade-cta h2 {
            color: #ffffff;
            font-size: 28px;
            letter-spacing: -0.2px;
            margin: 0;
            padding: 0;
        }

        .gh-post-upgrade-cta p {
            margin: 20px 0 0;
            padding: 0;
        }

        .gh-post-upgrade-cta small {
            font-size: 16px;
            letter-spacing: -0.2px;
        }

        .gh-post-upgrade-cta a {
            color: #ffffff;
            cursor: pointer;
            font-weight: 500;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a:hover {
            color: #ffffff;
            opacity: 0.8;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a.gh-btn {
            display: block;
            background: #ffffff;
            text-decoration: none;
            margin: 28px 0 0;
            padding: 8px 18px;
            border-radius: 4px;
            font-size: 16px;
            font-weight: 600;
        }

        .gh-post-upgrade-cta a.gh-btn:hover {
            opacity: 0.92;
        }
    </style>
    <script defer src="./assets/js/cards.min.js"></script>
    <style>
        :root {
            --ghost-accent-color: #15171A;
        }
    </style>
    <link rel="stylesheet" type="text/css" href="./assets/css/cards.min.css">
</head>

<body>

    <header>
        <nav class="nav container" data-url="/blog/">
            <div class="nav-row row align-items-center">
                <div class="d-none d-sm-block col-sm nav-symbol-wrap">
                    <a href="./index.html"> Home </a>
                </div>
                <div class="col col-sm-auto">
                    <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
                        <div class="d-sm-none nav-symbol-wrap">
                        </div>
                        
                        <!--li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="overview.html"
                                data-slug="research">Overview</a></li-->

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="research.html"
                            data-slug="research">Research</a></li>

                    <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="blog.html"
                            data-slug="blog">Blog</a></li>

                    <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link active" href="about.html"
                            data-slug="about">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>

    </header>
    <div class="container mt-4" style="text-align:justify">
        <p>
        Our long-term mission is to advance <i>AI</i> <i>for humanity</i>.  We are dedicated to pioneering <b>a new paradigm of Generative AI</b>, focusing on both technologies and models. Previously, we focused on scaling recipes with paradigm-shifting technologies for new generations of foundation models. <font style="padding-left:0px; padding-top:10px; font-size:13pt">[Last Update: January 2025]</font>
        </p>
        <br>
        
         <p>
        ==
        <br>
        </b>
        
        <p>
        <b><i>The Next Recipe</i></b> [08/2024 - ]
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>R1.1</b> (Efficiency Revolution of AI): <b>BitNet</b> (b1.58 | a4.8) + MH-MoE + YOCO</li>
        <li><b>R1.2</b> (Transformer in the Post-Transformer Era: Generally better with less hallucination & better long-sequence reasoning): <b>DIFF Transformer</b></li>
        <li><b>R2.1</b> (A New Paradigm of Multimodality): <b>LatentLM</b></li>
        <li><b>R2.2</b>: coming soon</li>
        <li><b>R3</b> (New Generation of (M)LLMs and Generative AI):  R1.1 + R1.2 + R2.1 + R2.2</li>
         
        </ul>
        </p>
        <br>
        
        <p>
        ==
        <br>
        </b>
        
        <p>
        <b><i>The Second Curve of Scaling Law</i></b>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>Scaling Law</b>:<font color="red"> <a href="./assets/doc/The Second Curve of Scaling Law_Furu Wei_public.pdf">The Second Curve of Scaling Law</a></font></li>
        <b>Model Architecture - (The Revolution of Model) Efficiency</b>
        <li>&nbsp;&nbsp;&nbsp;&nbsp;<b>The Era of 1-bit LLMs</b>:<font color="red"> <a href="https://arxiv.org/abs/2402.17764">(BitNet b1.58) All Large Language Models are in 1.58 Bits</a> <font color="black">|</font> <a href="https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf">Training Tips, Code and FAQ</a></font></li>
        <li>&nbsp;&nbsp;&nbsp;&nbsp;<b>Fully & Extremely Sparsely-Activated LLMs</b>:<font color="red"> <a href="https://arxiv.org/abs/2407.10969">(Block) Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a></font></li>
        <li>&nbsp;&nbsp;&nbsp;&nbsp;<b>You Only Cache Once</b>:<font color="red"> <a href="https://arxiv.org/abs/2405.05254">You Only Cache Once: Decoder-Decoder Architectures for Language Models</a> <font color="black">|</font> <a href="https://arxiv.org/abs/2405.05254">Gated RetNet / RetNet-3</a></font></li>
        <b>Model Architecture - Capability</b>
        <li>&nbsp;&nbsp;&nbsp;&nbsp;<b>DIFF</b>:<font color="red"> <a href="https://arxiv.org/abs/2410.05258">Differential Transformer</a></font></li>
        <li><b>The Learning Law</b>:<font color="red"> <a href="https://arxiv.org/abs/2402.17759">Towards Optimal Learning of Language Models</a></font></li>
        <li><i>More (coming)</i></li>
        </ul>
        </p>
        <br>
        
        <p>
        ==
        <br>
        </b>
        
        <p>
        <i>Foundation Models</i>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>The Evolution of (M)LLMs</b>:<br><font color="red">Kosmos-1/2/2.5/G</font>, <font color="red">VALL-E</font></li>
        <br>
        <li>#<b>TheBigConvergence</b> of foundation models and large-scale self-supervised pre-training across tasks, languages, and modalities.</li>
        <li>Language: <font color="blue">UniLM</font>(-2)</li>
        <li>Multilingual: InfoXLM, <font color="blue">XLM-E</font></li>
        <li>Vision: <font color="blue">BEiT</font>(-2)</li>
        <li>Speech: WavLM, SpeechLM, <font color="blue">VALL-E</font></li>
        <li>Multimodal: VLMo, <font color="blue">BEiT-3</font></li>
        <li>Document (Multimodal): <font color="blue">Layout(X)LM(-2/3)</font></li>
        <li>General-purpose: <font color="blue">MetaLM</font></li>
        
        </ul>
        </p>
        <br>
        
        <p>
        <i>Foundation Architecture</i>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>The Revolution of Model Architecture</b>:<br><font color="red">RetNet</font>, <font color="red">BitNet</font>, <font color="red">LongNet</font><br><font color="red">TorchScale</font> (<b>Library</b>)</li>
        <br>
        <li>Fundamental research on modeling generality and capability, as well as training stability and efficiency.</li>
        <li><font color="blue">DeepNet</font>: training stability</li>
        <li><font color="blue">Magneto</font>: modeling generality</li>
        <li><font color="blue">Multiway Transformers</font>: multimodal modeling</li>
        <li>X-MoE: efficiency</li>
        <li><font color="blue">LEX Transformer</font>: better position embedding and length extrapolation</li>
        </ul>
        </p>
        <br>
        
        <p>
        <i>Science of Intelligence</i>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li>Understanding the principles and theoretical boundary of (artificial general) intelligence</li>
        <li><a href="https://arxiv.org/abs/2212.10559" target=_blank><font color="blue">Why Can GPT Learn In-Context?</font></a></li>
        </ul>
        
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>The Learning Law</b></li>
        <li><font color="red"> <a href="https://arxiv.org/abs/2402.17759">Towards Optimal Learning of Language Models</a></font></li>
        </ul>
        </p>
        <br>
        
        <p>
        <i>LLMOps</i>: Research and technology for building AI products w/ foundation models. 
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li>General technology for <b>enabling AI capabilities w/ (M)LLMs</b></li>
        <li><font color="red">MiniLLM</font>: LLM Distillation</li>
        <li><font color="blue">AdaLLM: LLM Adaptation</font></li>
        <li><font color="blue">LLM Accelerator</font></li>
        <li>Promptist, Extensible Prompts, Structured Prompting</li>
        <br>
        <li>Effective and efficient approaches to deploying large AI models in practice</li>
        <li><font color="blue">MiniLM</font>(-2), <font color="blue">xTune</font>, EdgeFormer, <font color="blue">Aggressive Decoding</font></li>
        </ul>
        </p>
        <br>
        <p>
        In addition to the research achievements, these models are significant parts of Microsoft's own family of large AI (foundation) models powering language and multimodal tasks and scenarios across products in Microsoft. Moreover, our research tops public benchmarks and leaderboards across language, vision, speech, and multimodal tasks, and hugely contributes to the open source community through GitHub and Hugging Face.</p>
       
       <br>
       <p>More information about our <a href="research.html"><font color="blue">Research</font></a><!-- and <a href="overview.html"><font color="blue">Highlights</font></a>-->.</p>
        
        <br>
          <p> <a href="https://github.com/microsoft/unilm" target="_blank"><font color="blue">microsoft/unilm</font>: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities</a> </p>
          <p> <a href="https://github.com/microsoft/BitNet" target="_blank"><font color="blue">microsoft/BitNet</font>: 1-bit AI Infra: Official inference framework for 1-bit LLMs</a> </p>
        <!--p> <a href="https://github.com/microsoft/torchscale" target="_blank"><font color="blue">microsoft/torchscale</font>: Neural Architecture for General AI</a> </p>
        <p> <a href="https://github.com/microsoft/LMOps" target="_blank"><font color="blue">microsoft/lmops</font>: General technology for enabling AI capabilities w/ (M)LLMs</a> </p-->
       
        <br>
        
        <p>
            We are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models and General AI, NLP, MT, Speech, Document AI and Multimodal AI, 
            please send your resume to <a href = "mailto: fuwei@microsoft.com"><u>fuwei@microsoft.com</u></a>.
        </p>
        <br>
        
    </div>



    <footer class="footer container medium-xsmall-copy line-height-1.6">

        <div class="row align-items-center mb-0.125">
            <div class="col-12 col-md mb-0.5">
                <a class="fade" style="margin-top:1px" href="/">&copy; 2022</a>
            </div>
        </div>
    </footer>

    <script type="text/javascript" src="./assets/js/main.min.js"></script>


</body>

</html>
