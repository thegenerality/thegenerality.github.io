<!DOCTYPE html>
<html lang="en">

<head>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-71156606-1');
    </script>
    <meta charset="utf-8" />
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Advancing AI for humanity | Foundation of AI</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="./assets/css/main.css" />

    <script type="text/javascript">document.documentElement.className = 'js';</script>
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:image:width" content="3200" />
    <meta property="og:image:height" content="1800" />

    <meta name="generator" content="Ghost 5.12" />
    <style id="gh-members-styles">
        .gh-post-upgrade-cta-content,
        .gh-post-upgrade-cta {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            text-align: center;
            width: 100%;
            color: #ffffff;
            font-size: 16px;
        }

        .gh-post-upgrade-cta-content {
            border-radius: 8px;
            padding: 40px 4vw;
        }

        .gh-post-upgrade-cta h2 {
            color: #ffffff;
            font-size: 28px;
            letter-spacing: -0.2px;
            margin: 0;
            padding: 0;
        }

        .gh-post-upgrade-cta p {
            margin: 20px 0 0;
            padding: 0;
        }

        .gh-post-upgrade-cta small {
            font-size: 16px;
            letter-spacing: -0.2px;
        }

        .gh-post-upgrade-cta a {
            color: #ffffff;
            cursor: pointer;
            font-weight: 500;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a:hover {
            color: #ffffff;
            opacity: 0.8;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a.gh-btn {
            display: block;
            background: #ffffff;
            text-decoration: none;
            margin: 28px 0 0;
            padding: 8px 18px;
            border-radius: 4px;
            font-size: 16px;
            font-weight: 600;
        }

        .gh-post-upgrade-cta a.gh-btn:hover {
            opacity: 0.92;
        }
    </style>
    <script defer src="./assets/js/cards.min.js"></script>
    <style>
        :root {
            --ghost-accent-color: #15171A;
        }
    </style>
    <link rel="stylesheet" type="text/css" href="./assets/css/cards.min.css">
</head>

<body>

    <header>
        <nav class="nav container" data-url="/blog/">
            <div class="nav-row row align-items-center">
                <div class="d-none d-sm-block col-sm nav-symbol-wrap">
                    <a href="./index.html"> Home </a>
                </div>
                <div class="col col-sm-auto">
                    <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
                        <div class="d-sm-none nav-symbol-wrap">
                        </div>
                        
                        <!--li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="overview.html"
                                data-slug="research">Overview</a></li-->

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="research.html"
                            data-slug="research">Research</a></li>

                    <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="blog.html"
                            data-slug="blog">Blog</a></li>

                    <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link active" href="about.html"
                            data-slug="about">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>

    </header>
    <div class="container mt-4" style="text-align:justify">
        <p>
        Our long-term mission is to advance <i>AI</i> <i>for humanity</i>. We focus on fundamental and disruptive research of the <b>Foundation of AI</b>. We are also committed to building the <i>new foundation of AI</i>, including the <b>New Generation of Foundation Models</b>.
        </p>
        <br>
        
        <p>
        <b><i>The Second Curve of Scaling Law</i></b>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>Scaling Law</b>:<font color="red"> <a href="./assets/doc/The Second Curve of Scaling Law_Furu Wei_public.pdf">The Second Curve of Scaling Law</a></font></li>
        <li><b>The Era of 1-bit LLMs</b>:<font color="red"> <a href="https://arxiv.org/abs/2402.17764">(BitNet b1.58) All Large Language Models are in 1.58 Bits</a> <font color="black">|</font> <a href="https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf">Training Tips, Code and FAQ</a></font></li>
        <li><b>Model Architecture</b>:<font color="red"> <a href="https://arxiv.org/abs/2405.05254">You Only Cache Once</a> <font color="black">|</font> <a href="https://arxiv.org/abs/2405.05254">Gated RetNet / RetNet-3</a></font></li>
        <li><b>The Learning Law</b>:<font color="red"> <a href="https://arxiv.org/abs/2402.17759">Towards Optimal Learning of Language Models</a></font></li>
        <li><i>More (coming)</i></li>
        </ul>
        </p>
        <br>
        
        <p>
        <i>Foundation Models</i>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>The Evolution of (M)LLMs</b>:<br><font color="red">Kosmos-1/2/2.5/G</font>, <font color="red">VALL-E</font></li>
        <br>
        <li>#<b>TheBigConvergence</b> of foundation models and large-scale self-supervised pre-training across tasks, languages, and modalities.</li>
        <li>Language: <font color="blue">UniLM</font>(-2)</li>
        <li>Multilingual: InfoXLM, <font color="blue">XLM-E</font></li>
        <li>Vision: <font color="blue">BEiT</font>(-2)</li>
        <li>Speech: WavLM, SpeechLM, <font color="blue">VALL-E</font></li>
        <li>Multimodal: VLMo, <font color="blue">BEiT-3</font></li>
        <li>Document (Multimodal): <font color="blue">Layout(X)LM(-2/3)</font></li>
        <li>General-purpose: <font color="blue">MetaLM</font></li>
        
        </ul>
        </p>
        <br>
        
        <p>
        <i>Foundation Architecture</i>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>The Revolution of Model Architecture</b>:<br><font color="red">RetNet</font>, <font color="red">BitNet</font>, <font color="red">LongNet</font><br><font color="red">TorchScale</font> (<b>Library</b>)</li>
        <br>
        <li>Fundamental research on modeling generality and capability, as well as training stability and efficiency.</li>
        <li><font color="blue">DeepNet</font>: training stability</li>
        <li><font color="blue">Magneto</font>: modeling generality</li>
        <li><font color="blue">Multiway Transformers</font>: multimodal modeling</li>
        <li>X-MoE: efficiency</li>
        <li><font color="blue">LEX Transformer</font>: better position embedding and length extrapolation</li>
        </ul>
        </p>
        <br>
        
        <p>
        <i>Science of Intelligence</i>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li>Understanding the principles and theoretical boundary of (artificial general) intelligence</li>
        <li><a href="https://arxiv.org/abs/2212.10559" target=_blank><font color="blue">Why Can GPT Learn In-Context?</font></a></li>
        </ul>
        
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>The Learning Law</b></li>
        <li><font color="red"> <a href="https://arxiv.org/abs/2402.17759">Towards Optimal Learning of Language Models</a></font></li>
        </ul>
        </p>
        <br>
        
        <p>
        <i>LLMOps</i>: Research and technology for building AI products w/ foundation models. 
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li>General technology for <b>enabling AI capabilities w/ (M)LLMs</b></li>
        <li><font color="red">MiniLLM</font>: LLM Distillation</li>
        <li><font color="blue">AdaLLM: LLM Adaptation</font></li>
        <li><font color="blue">LLM Accelerator</font></li>
        <li>Promptist, Extensible Prompts, Structured Prompting</li>
        <br>
        <li>Effective and efficient approaches to deploying large AI models in practice</li>
        <li><font color="blue">MiniLM</font>(-2), <font color="blue">xTune</font>, EdgeFormer, <font color="blue">Aggressive Decoding</font></li>
        </ul>
        </p>
        <br>
        <p>
        In addition to the research achievements, these models are significant parts of Microsoft's own family of large AI (foundation) models powering language and multimodal tasks and scenarios across products in Microsoft. Moreover, our research tops public benchmarks and leaderboards across language, vision, speech, and multimodal tasks, and hugely contributes to the open source community through GitHub and Hugging Face.</p>
       
       <br>
       <p>More information about our <a href="research.html"><font color="blue">Research</font></a> and <a href="overview.html"><font color="blue">Highlights</font></a>.</p>
        
        <br>
          <p> <a href="https://github.com/microsoft/unilm" target="_blank"><font color="blue">microsoft/unilm</font>: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities</a> </p>
        <p> <a href="https://github.com/microsoft/torchscale" target="_blank"><font color="blue">microsoft/torchscale</font>: Neural Architecture for General AI</a> </p>
        <p> <a href="https://github.com/microsoft/LMOps" target="_blank"><font color="blue">microsoft/lmops</font>: General technology for enabling AI capabilities w/ (M)LLMs</a> </p>
       
        <br>
        
        <p>
            We are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models and General AI, NLP, MT, Speech, Document AI and Multimodal AI, 
            please send your resume to <a href = "mailto: fuwei@microsoft.com"><u>fuwei@microsoft.com</u></a>.
        </p>
        <br>
        
    </div>



    <footer class="footer container medium-xsmall-copy line-height-1.6">

        <div class="row align-items-center mb-0.125">
            <div class="col-12 col-md mb-0.5">
                <a class="fade" style="margin-top:1px" href="/">&copy; 2022</a>
            </div>
        </div>
    </footer>

    <script type="text/javascript" src="./assets/js/main.min.js"></script>


</body>

</html>
