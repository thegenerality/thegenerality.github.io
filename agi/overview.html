<!DOCTYPE html>
<html lang="en">

<head>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-71156606-1');
    </script>
    <meta charset="utf-8" />
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Advancing AI for humanity | Foundation of AI</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="./assets/css/main.css" />

    <script type="text/javascript">document.documentElement.className = 'js';</script>
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:image:width" content="3200" />
    <meta property="og:image:height" content="1800" />

    <meta name="generator" content="Ghost 5.12" />
    <style id="gh-members-styles">
        .gh-post-upgrade-cta-content,
        .gh-post-upgrade-cta {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            text-align: center;
            width: 100%;
            color: #ffffff;
            font-size: 16px;
        }

        .gh-post-upgrade-cta-content {
            border-radius: 8px;
            padding: 40px 4vw;
        }

        .gh-post-upgrade-cta h2 {
            color: #ffffff;
            font-size: 28px;
            letter-spacing: -0.2px;
            margin: 0;
            padding: 0;
        }

        .gh-post-upgrade-cta p {
            margin: 20px 0 0;
            padding: 0;
        }

        .gh-post-upgrade-cta small {
            font-size: 16px;
            letter-spacing: -0.2px;
        }

        .gh-post-upgrade-cta a {
            color: #ffffff;
            cursor: pointer;
            font-weight: 500;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a:hover {
            color: #ffffff;
            opacity: 0.8;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a.gh-btn {
            display: block;
            background: #ffffff;
            text-decoration: none;
            margin: 28px 0 0;
            padding: 8px 18px;
            border-radius: 4px;
            font-size: 16px;
            font-weight: 600;
        }

        .gh-post-upgrade-cta a.gh-btn:hover {
            opacity: 0.92;
        }
    </style>
    <script defer src="./assets/js/cards.min.js"></script>
    <style>
        :root {
            --ghost-accent-color: #15171A;
        }
        tag {
            /* color: -webkit-link; */
            font-style: italic;
            font-size: 16px;
            color: black;
            /* color: white;
            background-color: #264653!important;
            font-weight: 600;
            padding: 0px 3px 0 3px;
            margin-right: 5px; */
        }
    </style>
    <link rel="stylesheet" type="text/css" href="./assets/css/cards.min.css">
</head>

<body>

    <header>
        <nav class="nav container" data-url="/blog/">
            <div class="nav-row row align-items-center">
                <div class="d-none d-sm-block col-sm nav-symbol-wrap">
                    <a href="./index.html"> Home </a>
                </div>
                <div class="col col-sm-auto">
                    <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
                        <div class="d-sm-none nav-symbol-wrap">
                        </div>
                        
                        <!--li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link active" href="overview.html"
                                data-slug="research">Overview</a></li-->

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="research.html"
                            data-slug="research">Research</a></li>

                    <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="blog.html"
                            data-slug="blog">Blog</a></li>

                    <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="about.html"
                            data-slug="about">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>

    </header>
    <div class="container mt-4">
        <h2 class="mb-2">Research Highlights</h2>
        
        <p style="padding-bottom:30px">Our mission-focused research agenda of advancing AI for humanity.</p>
        
        
        <p>
        <b><i>The Second Curve of Scaling Law</i></b>
        <br>
        <ul style="padding-left:30px; padding-top:10px; font-size:13pt">
        <li><b>Scaling Law</b>:<font color="red"> <a href="./assets/doc/The Second Curve of Scaling Law_Furu Wei_public.pdf">The Second Curve of Scaling Law</a></font></li>
        <li><b>The Era of 1-bit LLMs</b>:<font color="red"> <a href="https://arxiv.org/abs/2402.17764">(BitNet b1.58) All Large Language Models are in 1.58 Bits</a> <font color="black">|</font> <a href="https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf">Training Tips, Code and FAQ</a></font></font></li>
        <li><b>Model Architecture</b>:<font color="red"> <a href="https://arxiv.org/abs/2405.05254">You Only Cache Once</a> <font color="black">|</font> <a href="https://arxiv.org/abs/2405.05254">Gated RetNet / RetNet-3</a></font></li>
        <li><b>The Learning Law</b>:<font color="red"> <a href="https://arxiv.org/abs/2402.17759">Towards Optimal Learning of Language Models</a></font></li>
        <li><i>More (coming)</i></li>
        </ul>
        </p>
        <br>
       
        <p style="padding-bottom:0px"><b>Foundation of AI</b></p>
        <br>
        
        <p style="padding-top:0px; padding-bottom:10px"><i>Foundation Models</i></p>

         <p style="padding-top:0px; padding-bottom:10px"><i>The Evolution of LLM / MLLM (Multimodal LLM)</i></p>
         
         <p>
         <div style="padding-left:20px; padding-top:10px">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2310.02992"><font color="red">Kosmos</font>-G: Generating Images in Context with Multimodal Large Language Models</a>. <tag>ICLR'24</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2309.11419"><font color="red">Kosmos</font>-2.5: A Multimodal Literate Model</a></h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2306.14824"><font color="red">Kosmos</font>-2: Grounding Multimodal Large Language Models (MLLMs) to the World</a>. <tag>ICLR'24</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2302.14045">(<font color="red">Kosmos</font>-1) Language Is Not All You Need: Aligning Perception with Language Models</a>. <tag>NeurIPS'23</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2206.06336">(MetaLM) Language Models are General-Purpose Interfaces</a></h5>
         
         <br>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2301.02111">(<font color="red">VALL-E</font>) Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a></h5>
         </div>
         </p>
         
        <br>
        
        <p style="padding-top:0px; padding-bottom:10px"><i><b>#TheBigConvergence</b> of <i>foundation models</i> and <i>large-scale pre-training across tasks, languages, and modalities</i></p>

        <p>
         <div style="padding-left:20px; padding-top:10px">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2208.10442">(<font color="blue">BEiT-3</font>) <font color="blue">Image as a Foreign Language</font>: BEiT Pretraining for All Vision and Vision-Language Tasks</a>. <tag>CVPR'23</tag>.</h5>
         <p style="height: 5px"></p>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2208.06366"><font color="blue">BEiT</font> v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers</a></h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2106.08254"><font color="blue">BEiT</font>: BERT Pre-Training of Image Transformers.</a> <tag>ICLR'22 (<font color="blue"><a href="https://openreview.net/forum?id=p-BhZSz59o4">Oral</a></font>)</tag>.</h5>
         <p style="height: 5px"></p>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2106.16138">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA.</a> <tag>ACL'22</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2104.08692">mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs.</a> <tag>EMNLP'21</tag>.</h5>
                  
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2007.07834">InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training.</a> <tag>NAACL'21</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2002.12804">UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training.</a> <tag>ICML'20</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/1905.03197">(UniLM) Unified Language Model Pre-training for Natural Language Understanding and Generation.</a> <tag>NeurIPS'19</tag>.</h5>
         </div>
         </p>
        
        <br>
        
        <p style="padding-top:0px; padding-bottom:10px"><i>Foundation Architecture</i></p>
        
        <p style="padding-left:0px; padding-bottom:10px"><i>The Revolution of Model Architecture</i></p>
               
        <p>
         <div style="padding-left:20px; padding-top:0">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2310.11453"><font color="red">BitNet</font>: Scaling 1-bit Transformers for Large Language Models</a></h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2307.08621">(<font color="red">RetNet</font>) Retentive Network: A Successor to Transformer for Large Language Models</a></h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2307.02486"><font color="red">LongNet</font>: Scaling Transformers to 1,000,000,000 Tokens</a></h5>
         </div>
         </p>
        
        <p style="padding-left:0px; padding-top:20px"><i>Fundamental research on modeling generality and capability, as well as training stability and efficiency</i></p>
        <p>
         <div style="padding-left:20px; padding-top:10px">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2211.13184"><font color="red">TorchScale</font>: Transformers at Scale</a></h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2309.10400">(<font color="blue">PoSE</font>) Efficient Context Window Extension of LLMs via Positional Skip-wise Training</a>. <tag>ICLR'24</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2212.10554">(<font color="blue">XPos</font>) A Length-Extrapolatable Transformer</a>. <tag>ACL'23</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2210.06423">Magneto: A <font color="blue">Foundation Transformer</font>.</a> <tag>ICML'23</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2203.00555">DeepNet: Scaling Transformers to 1,000 Layers.</a> <tag>TPAMI, 2024</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2204.09179">On the Representation Collapse of Sparse Mixture of Experts.</a> <tag>NeurIPS'22</tag>.</h5>
         </div>
         </p>
         
         <br>
         
        
        <p style="padding-top:0px; padding-bottom:10px"><i>Science of Intelligence</i></p>
        <p>
           Fundamental research to understand the principles and theoretical boundary of (artificial general) intelligence.
        </p>
        
        <p>
         <div style="padding-left:20px; padding-top:10px">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2212.10559"><font color="blue">Why Can GPT Learn In-Context</font>? Language Models Secretly Perform Gradient Descent as Meta Optimizers</a>. <tag>ACL'23</tag>.</h5>
         </div>
         </p>
         
         <br>
         
        <p style="padding-top:0px; padding-bottom:10px"><b>LLMOps</b></p>
        <p>
           General technology for <b>enabling AI capabilities w/ (M)LLMs</b>. 
        </p>

<div style="padding-left:0pt">
        <br>
        
        <p style="padding-bottom:10px"><i>LLM Adaptation</i></p>
               
        <p>
         <div style="padding-left:20px; padding-top:0">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2309.09530">(<font color="red">AdaLLM</font>) Adapting Large Language Models via Reading Comprehension</a>. <tag>ICLR'24</tag>.</h5>
         </div>
         </p>
         
         <br>
        
         <p style="padding-bottom:10px"><i>LLM Distillation</i></p>
               
        <p>
         <div style="padding-left:20px; padding-top:0">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2306.08543">(<font color="red">MiniLLM</font>) Knowledge Distillation of Large Language Models</a>. <tag>ICLR'24</tag>.</h5>
         </div>
         </p>
         
         <br>
        
        <p style="padding-bottom:10px"><i>LLM Accelerator</i></p>
               
        <p>
         <div style="padding-left:20px; padding-top:0">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2304.04487">(<font color="blue">llma</font>) Inference with Reference: <font color="blue">Lossless Acceleration of Large Language Models</font></a></h5>
         </div>
         </p>
         
         <br>
        
        <p style="padding-top:0px; padding-bottom:10px"><i>Prompt Intelligence</i></p>
        <p>
           Prompt as a new language of Foundation Models and Generative AI, and a new programming language and interface for Human-AI communication and collaboration. 
        </p>
        
        <p>
         <div style="padding-left:20px; padding-top:10px">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2307.07164">Learning to Retrieve In-Context Examples for Large Language Models</a>. <tag>EACL'24</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2305.07004">(Cross-Lingual-Thought) Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting</a>. <tag>EMNLP'23</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2212.09611">(Promptist) Optimizing Prompts for Text-to-Image Generation</a>. <tag>NeurIPS'23</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2212.06713"><font color="blue">Structured Prompting</font>: Scaling In-Context Learning to 1,000 Examples</a></h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2212.00616">Extensible Prompts for Language Models</a>. <tag>NeurIPS'23</tag>.</h5>
         </div>
         </p>

        <br>
        
        <p style="padding-bottom:10px"><i>Democratizing Foundation Models</i></p>
        <p>
           Research and development of effective and efficient approaches to deploying large AI (foundation) models in practice. 
        </p>
        
        <p>
         <div style="padding-left:20px; padding-top:10px">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2205.10350">Lossless Acceleration for Seq2seq Generation with Aggressive Decoding</a>. <tag>EMNLP'23</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2202.07959">EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation.</a> <tag>EMNLP'22</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2106.08226">(xTune) Consistency Regularization for Cross-Lingual Fine-Tuning.</a> <tag>EMNLP'21</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2012.15828"><font color="blue">MiniLM</font>v2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers.</a> <tag>ACL'21</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2002.10957"><font color="blue">MiniLM</font>: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a>.</a> <tag>NeurIPS'22</tag>.</h5>
         </div>
         </p>
         
         </div>
         
        <br>
        
        <br>
        <p style="padding-bottom:10px"><b>Verticals</b></p>
        <p>Our research also pushes disruptive technologies for vertical domains and/or tasks. </p>
        <br>
        
        <p style="padding-top:0px; padding-bottom:10px"><i>Revolutionizing Document AI</i></p>
        <p>
           Our pioneering research on multimodal document foundation models for technology evolution of Document AI. 
        </p>
        
        <p>
         <div style="padding-left:20px; padding-top:10px">
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2204.08387"><font color="blue">LayoutLM</font>v3: Pre-training for Document AI with Unified Text and Image Masking.</a> <tag>ACM MM'22</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2012.14740"><font color="blue">LayoutLM</font>v2: Multi-modal Pre-training for Visually-Rich Document Understanding.</a> <tag>ACL'21</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/1912.13318"><font color="blue">LayoutLM</font>: Pre-training of Text and Layout for Document Image Understanding.</a> <tag>KDD'20</tag>.</h5>
         
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2104.08836">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding.</a> <tag>ACL'21</tag>.</h5>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2203.02378">DiT: Self-supervised Pre-training for Document Image Transformer.</a> <tag>ACM MM'22</tag>.</h5>
         
         <p style="height: 5px"></p>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2110.08518">MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding.</a> <tag>ACL'22</tag>.</h5>
             
         <p style="height: 5px"></p>
         <h5 class="medium-xsmall-copy balance-text mb-1/12"><a href="https://arxiv.org/abs/2109.10282">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models.</a> <tag>AAAI'23</tag>.</h5>
         </div>
         </p>

    </div>


    <footer class="footer container medium-xsmall-copy line-height-1.6">

        <div class="row align-items-center mb-0.125">
            <div class="col-12 col-md mb-0.5">
                <a class="fade" style="margin-top:1px" href="/">&copy; 2022</a>
            </div>
        </div>
    </footer>

    <script type="text/javascript" src="./assets/js/main.min.js"></script>


</body>

</html>
