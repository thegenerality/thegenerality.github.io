<!DOCTYPE html>
<html lang="en">

<head>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-71156606-1');
    </script>
    <meta charset="utf-8" />
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Advancing AI for Humanity</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="./assets/css/main.css" />

    <script type="text/javascript">document.documentElement.className = 'js';</script>
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:image:width" content="3200" />
    <meta property="og:image:height" content="1800" />

    <meta name="generator" content="Ghost 5.12" />
    <style id="gh-members-styles">
        .gh-post-upgrade-cta-content,
        .gh-post-upgrade-cta {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            text-align: center;
            width: 100%;
            color: #ffffff;
            font-size: 16px;
        }

        .gh-post-upgrade-cta-content {
            border-radius: 8px;
            padding: 40px 4vw;
        }

        .gh-post-upgrade-cta h2 {
            color: #ffffff;
            font-size: 28px;
            letter-spacing: -0.2px;
            margin: 0;
            padding: 0;
        }

        .gh-post-upgrade-cta p {
            margin: 20px 0 0;
            padding: 0;
        }

        .gh-post-upgrade-cta small {
            font-size: 16px;
            letter-spacing: -0.2px;
        }

        .gh-post-upgrade-cta a {
            color: #ffffff;
            cursor: pointer;
            font-weight: 500;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a:hover {
            color: #ffffff;
            opacity: 0.8;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a.gh-btn {
            display: block;
            background: #ffffff;
            text-decoration: none;
            margin: 28px 0 0;
            padding: 8px 18px;
            border-radius: 4px;
            font-size: 16px;
            font-weight: 600;
        }

        .gh-post-upgrade-cta a.gh-btn:hover {
            opacity: 0.92;
        }
    </style>
    <script defer src="./assets/js/cards.min.js"></script>
    <style>
        :root {
            --ghost-accent-color: #15171A;
        }
        tag {
            /* color: -webkit-link; */
            font-style: italic;
            font-size: 16px;
            color: black;
            /* color: white;
            background-color: #264653!important;
            font-weight: 600;
            padding: 0px 3px 0 3px;
            margin-right: 5px; */
        }
        /* tag.conf {
            color: white;
            background-color: #2a9d8f!important;
            font-weight: 600;
            padding: 0px 3px 0 3px;
        } */
        
    </style>
    <link rel="stylesheet" type="text/css" href="./assets/css/cards.min.css">
</head>

<body>

    <header>
        <nav class="nav container" data-url="/blog/">
            <div class="nav-row row align-items-center">
                <div class="d-none d-sm-block col-sm nav-symbol-wrap">
                    <a href="./index.html"> Home </a>
                </div>
                <div class="col col-sm-auto">
                    <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
                        <div class="d-sm-none nav-symbol-wrap">
                        </div>
                        
                        <!--li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="overview.html"
                                data-slug="research">Overview</a></li-->

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link active" href="research.html"
                                data-slug="research">Research</a></li>

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="blog.html"
                                data-slug="blog">Blog</a></li>

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="about.html"
                                data-slug="about">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>

    </header>
    
   
    <div class="container mt-4">
        <h2 class="mb-2">Publication</h2>
        <!--p style="padding-bottom:20px"><a href="overview.html"><font color="blue">Highlights</font></a></p-->
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                 <div class="post-card-full-hide color-fg-50 mb-1.5">2025</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2506.04108>Rectified Sparse Attention</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2506.04108 class="color-fg-50">
                             Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42 end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference.<br>
                                <time datetime="2025-06-05">June 5, 2025</time>
                            </a>
                            <br>
                            <tag> Sparse Attention | Frontier Efficiency Model </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2505.23585>On-Policy RL with Optimal Reward Baseline</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2505.23585 class="color-fg-50">
                             In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks.<br>
                                <time datetime="2025-05-21">May 30, 2025</time>
                            </a>
                            <br>
                            <tag> RL in LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2505.14674>Reward Reasoning Model</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2505.14674 class="color-fg-50">
                            Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy.<br>
                                <time datetime="2025-05-21">May 21, 2025</time>
                            </a>
                            <br>
                            <tag> Frontier Efficiency Model | Adaptive Thinking Models | RL in LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/pdf/2505.14631>Think Only When You Need with Large Hybrid-Reasoning Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/pdf/2505.14631 class="color-fg-50">
                            We introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.<br>
                                <time datetime="2025-05-21">May 21, 2025</time>
                            </a>
                            <br>
                            <tag> Frontier Reward Model | RL in LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2504.18415>BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2504.18415 class="color-fg-50">
                               Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we pro-pose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.                               <br>
                                <time datetime="2025-04-28">April 28, 2025</time>
                            </a>
                            <br>
                            <tag> Frontier Efficiency Model | BitNet v2 | 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2504.12285>BitNet b1.58 2B4T Technical Report</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2504.12285 class="color-fg-50">
                               We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.
                               <br>
                                <time datetime="2025-04-15">April 15, 2025</time>
                            </a>
                            <br>
                            <tag> Frontier Efficiency Model | BitNet | 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=#>BitNet v2: Native 4-bit Activations for 1-bit LLMs</a></h5>
                        <div>
                            <a href=# class="color-fg-50">
                               Outliers in the activations pose significant challenges for low-bit activations of  Large Language Models (LLMs). In this work, we present BitNet v2, which enables native 4-bit activations for 1-bit LLMs. To address outlier channels in the intermediate states of attention and feed-forward-network layers, we propose H-BitLinear, which applies an online Hadamard transformation before activation quantization. This transformation reshapes the sharp distribution of intermediate states into a more Gaussian-like form. Extensive experiments show that BitNet v2 with 8-bit activations matches the performance of BitNet b1.58 when training from the scratch. Moreover, BitNet v2 can be further trained to support native 4-bit activations with minimal performance drop, substantially reducing the cost of batched inference.
                               <br>
                                <time datetime="2025-04-17">April 17, 2025</time>
                            </a>
                            <br>
                            <tag> BitNet | 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2503.19551>Scaling Laws of Synthetic Data for Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2503.19551 class="color-fg-50">
                               In this work, we systematically investigate the scaling laws of synthetic data by introducing SYNTHLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our extensive mathematical experiments on SYNTHLLM include: (1) SYNTHLLM generates synthetic data that reliably adheres to the rectified scaling law across various model sizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SYNTHLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to organic pre-training corpora, offering a viable path toward continued improvement in model performance.
                               <br>
                                <time datetime="2025-03-25">March 25, 2025</time>
                            </a>
                            <br>
                            <tag> Synthetic Data </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2503.21172>Model as a Game: On Numerical and Spatial Consistency for Generative Games</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2503.21172 class="color-fg-50">
                               In this paper, we revisit the paradigm of generative games to explore what truly constitutes a Model as a Game (MaaG) with a well-developed mechanism. We begin with an empirical study on ``Traveler'', a 2D game created by an LLM featuring minimalist rules yet challenging generative models in maintaining consistency. Based on the DiT architecture, we design two specialized modules: (1) a numerical module that integrates a LogicNet to determine event triggers, with calculations processed externally as conditions for image generation; and (2) a spatial module that maintains a map of explored areas, retrieving location-specific information during generation and linking new observations to ensure continuity. Experiments across three games demonstrate that our integrated modules significantly enhance performance on consistency metrics compared to baselines, while incurring minimal time overhead during inference.
                               <br>
                                <time datetime="2025-03-27">March 27, 2025</time>
                            </a>
                            <br>
                            <tag> Multimodality </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2501.14342>Chain-of-Retrieval Augmented Generation</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2501.14342 class="color-fg-50">
                               This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.
                               <br>
                                <time datetime="2025-01-24">January 24, 2025</time>
                            </a>
                            <br>
                            <tag> RAG | Search </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2501.11110>Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2501.11110 class="color-fg-50">
                               Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet they often rely on single-paradigm reasoning that limits their effectiveness across diverse tasks. In this paper, we introduce Chain-of-Reasoning (CoR), a novel unified framework that integrates multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers using different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy that allows models to progressively master these paradigms, culminating in the development of CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4 in theorem proving tasks and a 7.9% improvement over RL-based methods in arithmetic tasks. These results showcase the enhanced mathematical comprehensive ability of our model, achieving significant performance gains on specific tasks and enabling zero-shot generalization across tasks.
                               <br>
                                <time datetime="2025-01-19">January 19, 2025</time>
                            </a>
                            <br>
                            <tag> Reasoning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2025">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2501.07542>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2501.07542 class="color-fg-50">
                               Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.
                               <br>
                                <time datetime="2025-01-13">January 13, 2025</time>
                            </a>
                            <br>
                            <tag> Multimodal Reasoning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2024</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2412.08635>Multimodal Latent Language Modeling with Next-Token Diffusion</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2412.08635 class="color-fg-50">
                               Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop Ïƒ-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models.
                               <br>
                                <time datetime="2024-11-26">November 26, 2024</time>
                            </a>
                            <br>
                            <tag> LatentLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
            <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2412.18860>Bootstrap Your Own Context Length</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2412.18860 class="color-fg-50">
                               We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.
                               <br>
                                <time datetime="2024-12-25">December 25, 2024</time>
                            </a>
                            <br>
                            <tag> Long Sequence </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2411.16205>MH-MoE: Multi-Head Mixture-of-Experts</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2411.16205 class="color-fg-50">
                               Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by using the multi-head mechanism to collectively attend to information from various representation spaces within different experts. In this paper, we present a novel implementation of MH-MoE that maintains both FLOPs and parameter parity with sparse Mixture of Experts models. Experimental results on language models show that the new implementation yields quality improvements over both vanilla MoE and fine-grained MoE models. Additionally, our experiments demonstrate that MH-MoE is compatible with 1-bit Large Language Models (LLMs) such as BitNet.
                               <br>
                                <time datetime="2024-11-26">November 26, 2024</time>
                            </a>
                            <br>
                            <tag> MH-MoE | BitNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2411.16345>Preference Optimization for Reasoning with Pseudo Feedback</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2411.16345 class="color-fg-50">
                               <tag> ICLR'25 (<font color="blue">Spotlight</font>) </tag>Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited. In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated test cases. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case. We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.
                               <br>
                                <time datetime="2024-11-26">November 26, 2024</time>
                            </a>
                            <br>
                            <tag> Reasoning | Post-training </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2411.04965>BitNet a4.8: 4-bit Activations for 1-bit LLMs</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2411.04965 class="color-fg-50">
                               Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.
                               <br>
                                <time datetime="2024-11-08">November 8, 2024</time>
                            </a>
                            <br>
                            <tag> BitNet | 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2410.05258>Differential Transformer</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2410.05258 class="color-fg-50">
                               <tag> ICLR'25 (<font color="blue">Oral</font>) </tag>Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.
                               <br>
                                <time datetime="2024-10-07">October 7, 2024</time>
                            </a>
                            <br>
                            <tag> DIFF </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2410.16144>1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2410.16144 class="color-fg-50">
                               Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs in terms of speed and energy consumption. These developments also enable local LLM deployment across a broad range of devices. In this work, we introduce bitnet.cpp, a tailored software stack designed to unlock the full potential of 1-bit LLMs. Specifically, we develop a set of kernels to support fast and lossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments demonstrate that this http URL achieves significant speedups, ranging from 2.37x to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model sizes.
                               <br>
                                <time datetime="2024-07-15">July 15, 2024</time>
                            </a>
                            <br>
                            <tag> 1-bit AI Infra | BitNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2407.10969>Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2407.10969 class="color-fg-50">
                               We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and fine-tuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs.
                               <br>
                                <time datetime="2024-07-15">July 15, 2024</time>
                            </a>
                            <br>
                            <tag> Q-Sparse | Block Q-Sparse | BitNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2405.05254>You Only Cache Once: Decoder-Decoder Architectures for Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2405.05254 class="color-fg-50">
                               <tag> NeurIPS'24 (<font color="blue">Oral</font>) </tag>We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes.
                               <br>
                                <time datetime="2024-05-09">May 9, 2024</time>
                            </a>
                            <br>
                            <tag> YOCO | RetNet(-3) </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf>The Era of 1-bit LLMs: Training Tips, Code and FAQ</a></h5>
                        <div>
                            <a href=https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf class="color-fg-50">
                               We present details and tips for training 1-bit LLMs. We also provide additional experiments and results that were not reported and responses to questions regarding the ``The-Era-of-1-bit-LLM'' paper. Finally, we include the official PyTorch implementation of BitNet (b1.58 and b1) for future research and development of 1-bit LLMs.
                                <br>
                                <time datetime="2024-03-20">Mar 20, 2024</time>
                            </a>
                            <br>
                            <tag> BitNet | The Era of 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.17764>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.17764 class="color-fg-50">
                               Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.
                                <br>
                                <time datetime="2024-02-28">Feb 28, 2024</time>
                            </a>
                            <br>
                            <tag> BitNet | The Era of 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2407.08551>Autoregressive Speech Synthesis without Vector Quantization</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2407.08551 class="color-fg-50">
                               We present MELLE, a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens. (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language models VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling discrete codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm.
                                <br>
                                <time datetime="2024-07-11">Jul 11, 2024</time>
                            </a>
                            <br>
                            <tag> #Speech MELLE  VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2406.14491>Instruction Pre-Training: Language Models are Supervised Multitask Learners</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2406.14491 class="color-fg-50">
                               Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning.
                                <br>
                                <time datetime="2024-06-20">Jun 20, 2024</time>
                            </a>
                            <br>
                            <tag> LLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2406.19774>Direct Preference Knowledge Distillation for Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2406.19774 class="color-fg-50">
                               In this work, we propose Direct Preference Knowledge Distillation (DPKD) for LLMs. DPKD utilizes distribution divergence to represent the preference loss and implicit reward function. We re-formulate KD of LLMs into two stages: first optimizing and objective consisting of implicit reward and reverse KL divergence and then improving the preference probability of teacher outputs over student outputs. We conducted experiments and analysis on various datasets with LLM parameters ranging from 120M to 13B and demonstrate the broad applicability and effectiveness of our DPKD approach. Meanwhile, we prove the value and effectiveness of the introduced implicit reward and output preference in KD through experiments and theoretical analysis. The DPKD method outperforms the baseline method in both output response precision and exact match percentage.
                                <br>
                                <time datetime="2024-06-28">Jun 28, 2024</time>
                            </a>
                            <br>
                            <tag> LLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2406.05370>VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2406.05370 class="color-fg-50">
                               This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis.
                                <br>
                                <time datetime="2024-06-08">Jun 8, 2024</time>
                            </a>
                            <br>
                            <tag> VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.15045>Multi-Head Mixture-of-Experts</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.15045 class="color-fg-50">
                               <tag> NeurIPS'24 </tag>We propose Multi-Head Mixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each token into multiple sub-tokens. These sub-tokens are then assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form. The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting. Extensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE.

                                <br>
                                <time datetime="2024-04-23">Apr 23, 2024</time>
                            </a>
                            <br>
                            <tag> MoE </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2405.12130>MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2405.12130 class="color-fg-50">
                               Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.

                                <br>
                                <time datetime="2024-05-20">May 20, 2024</time>
                            </a>
                            <br>
                            <tag> MoRA </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.17759>Towards Optimal Learning of Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.17759 class="color-fg-50">
                               This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an "LM-training-as-lossless-compression" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods.
                                <br>
                                <time datetime="2024-02-28">Feb 28, 2024</time>
                            </a>
                            <br>
                            <tag> Learning Law </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.03622>Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.03622 class="color-fg-50">
                               <tag> NeurIPS'24 </tag>We propose Visualization-of-Thought (VoT) prompting to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate \textit{mental images} to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.
                                <br>
                                <time datetime="2024-04-04">Apr 4, 2024</time>
                            </a>
                            <br>
                            <tag> Spatial Reasoning | VoT</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.01230>LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.01230 class="color-fg-50">
                               This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly.
                                <br>
                                <time datetime="2024-04-04">Apr 4, 2024</time>
                            </a>
                            <br>
                            <tag> Strategic Reasoning | Agent</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.00656>WavLLM: Towards Robust and Adaptive Speech Large Language Model</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.00656 class="color-fg-50">
                               We introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. 
                                <br>
                                <time datetime="2024-03-31">Mar 31, 2024</time>
                            </a>
                            <br>
                            <tag> MLLM | Speech LLMs</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2403.02884>MathScale: Scaling Instruction Tuning for Mathematical Reasoning</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2403.02884 class="color-fg-50">
                               <tag>ICML'24 </tag>We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs. Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate.
                                <br>
                                <time datetime="2024-03-05">Mar 5, 2024</time>
                            </a>
                            <br>
                            <tag> Scaling | Instruction Tuning</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.18039>ResLoRA: Identity Residual Mapping in Low-Rank Adaption</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.18039 class="color-fg-50">
                               Updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA.
                                <br>
                                <time datetime="2024-02-28">Feb 28, 2024</time>
                            </a>
                            <br>
                            <tag> LLM</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.16438>Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.16438 class="color-fg-50">
                               In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs.
                                <br>
                                <time datetime="2024-02-26">Feb 26, 2024</time>
                            </a>
                            <br>
                            <tag> LLM</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.13064>Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.13064 class="color-fg-50">
                               We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.
                                <br>
                                <time datetime="2024-02-20">Feb 20, 2024</time>
                            </a>
                            <br>
                            <tag> GLAN </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.05672>Multilingual E5 Text Embeddings: A Technical Report</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.05672 class="color-fg-50">
                               This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes.
                                <br>
                                <time datetime="2024-02-02">Feb 2, 2024</time>
                            </a>
                            <br>
                            <tag> E5 embeddings </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://browse.arxiv.org/abs/2402.01521>K-Level Reasoning with Large Language Models</a></h5>
                        <div>
                            <a href=https://browse.arxiv.org/abs/2402.01521 class="color-fg-50">
                               We propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on available historical information, which significantly improves the prediction accuracy of rivals' subsequent moves and informs more strategic decision-making. This research not only sets a robust quantitative benchmark for the assessment of dynamic reasoning but also markedly enhances the proficiency of LLMs in dynamic contexts.
                                <br>
                                <time datetime="2024-02-02">Feb 2, 2024</time>
                            </a>
                            <br>
                            <tag> Agent Reasoning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2401.00368>Improving Text Embeddings with Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2401.00368 class="color-fg-50">
                               <tag>ACL'24 </tag>In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.
                                <br>
                                <time datetime="2023-12-31">Dec 31, 2023</time>
                            </a>
                            <br>
                            <tag> E5 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2312.03558>When an Image is Worth 1,024 Ã— 1,024 Words: A Case Study in Computational Pathology</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2312.03558 class="color-fg-50">
                               We present LongViT, a vision Transformer that can process gigapixel images in an end-to-end manner. Specifically, we split the gigapixel image into a sequence of millions of patches and project them linearly into embeddings. LongNet is then employed to model the extremely long sequence, generating representations that capture both short-range and long-range dependencies. The linear computation complexity of LONGNET, along with its distributed algorithm, enables us to overcome the constraints of both computation and memory. 
                                <br>
                                <time datetime="2023-12-07">Dec 7, 2023</time>
                            </a>
                            <br>
                            <tag> LongViT LongNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2311.16465>TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2311.16465 class="color-fg-50">
                               <tag>ECCV'24 (<font color="blue">Oral</font>) </tag>We present TextDiffuser-2, aiming to unleash the power of language models for text rendering. Firstly, we fine-tune a large language model for layout planning. The large language model is capable of automatically generating keywords for text rendering and also supports layout modification through chatting. Secondly, we utilize the language model within the diffusion model to encode the position and texts at the line level. Unlike previous methods that employed tight character-level guidance, this approach generates more diverse text images. 
                                <br>
                                <time datetime="2023-11-29">Nov 29, 2023</time>
                            </a>
                            <br>
                            <tag> TextDiffuser </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2311.03220>ALYMPICS: Language Agents Meet Game Theory</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2311.03220 class="color-fg-50">
                               This paper introduces Alympics, a platform that leverages Large Language Model (LLM) agents to facilitate investigations in game theory. By employing LLMs and autonomous agents to simulate human behavior and enable multi-agent collaborations, we can construct realistic and dynamic models of human interactions for game theory hypothesis formulating and testing.
                                <br>
                                <time datetime="2023-11-06">Nov 6, 2023</time>
                            </a>
                            <br>
                            <tag> #Agent ALYMPICS </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2310.11453>BitNet: Scaling 1-bit Transformers for Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2310.11453 class="color-fg-50">
                               We introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.
                                <br>
                                <time datetime="2023-10-18">Oct 18, 2023</time>
                            </a>
                            <br>
                            <tag> BitNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2310.13385>Tuna: Instruction Tuning using Feedback from Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2310.13385 class="color-fg-50">
                               <tag>EMNLP'23 </tag>We propose finetuning an instruction-tuned LLM using our novel probabilistic ranking and contextual ranking approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs.
                                <br>
                                <time datetime="2023-10-20">Oct 20, 2023</time>
                            </a>
                            <br>
                            <tag> #Instruction_Tuning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2310.02992>Kosmos-G: Generating Images in Context with Multimodal Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2310.02992 class="color-fg-50">
                                <tag>ICLR'24 </tag>This paper presents KOSMOS-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. KOSMOS-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit KOSMOS-G as an initial attempt towards the goal of â€œimage as a foreign language in image generation.â€
                                <br>
                                <time datetime="2023-09-20">Sep 20, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-G </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.11419>Kosmos-2.5: A Multimodal Literate Model</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.11419 class="color-fg-50">
                                We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. 
                                <br>
                                <time datetime="2023-09-20">Sep 20, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-2.5 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.05689>Large Language Model for Science: A Study on P vs. NP</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.05689 class="color-fg-50">
                                We propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding "P â‰  NP". The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.
                                <br>
                                <time datetime="2023-09-13">Sep 13, 2023</time>
                            </a>
                            <br>
                            <tag> LLM4S </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        

        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.08621>Retentive Network: A Successor to Transformer for Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.08621 class="color-fg-50">
                               We propose retentive network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. The intriguing properties make
RetNet a strong successor to Transformer for large language models.
                                <br>
                                <time datetime="2023-07-18">July 18, 2023</time>
                            </a>
                            <br>
                            <tag> RetNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.02486>LongNet: Scaling Transformers to 1,000,000,000 Tokens</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.02486 class="color-fg-50">
                               We introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. 
                                <br>
                                <time datetime="2023-07-06">July 6, 2023</time>
                            </a>
                            <br>
                            <tag> LongNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.10400>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.10400 class="color-fg-50">
                               <tag>ICLR'24 </tag>We introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. 
                                <br>
                                <time datetime="2023-09-19">September 19, 2023</time>
                            </a>
                            <br>
                            <tag> #Long-context-modeling </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://openreview.net/forum?id=uWvKBCYh4S>MoLE: Mixture of LoRA Experts</a></h5>
                        <div>
                            <a href=https://openreview.net/forum?id=uWvKBCYh4S class="color-fg-50">
                               <tag>ICLR'24 </tag>This paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision & Language (V&L) domains substantiate the efficacy of MoLE. 
                                <br>
                                <time datetime="2023-09-18">September 18, 2023</time>
                            </a>
                            <br>
                            <tag>  </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.09530>Adapting Large Language Models via Reading Comprehension</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.09530 class="color-fg-50">
                               <tag>ICLR'24 </tag>We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. 
                                <br>
                                <time datetime="2023-09-18">September 18, 2023</time>
                            </a>
                            <br>
                            <tag>  </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2308.12770>WavMark: Watermarking for Audio Generation</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2308.12770 class="color-fg-50">
                               This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. 
                                <br>
                                <time datetime="2023-08-24">August 24, 2023</time>
                            </a>
                            <br>
                            <tag> #speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
                
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.07164>Learning to Retrieve In-Context Examples for Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.07164 class="color-fg-50">
                               <tag>EACL'24 </tag>We propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. 
                                <br>
                                <time datetime="2023-07-14">July 14, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.06945>In-context Autoencoder for Context Compression in a Large Language Model</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.06945 class="color-fg-50">
                               <tag>ICLR'24 </tag>We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with 4Ã— context compression, which can be well conditioned on by the target LLM to respond to various prompts. 
                                <br>
                                <time datetime="2023-07-13">July 13, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.05300>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.05300 class="color-fg-50">
                              We propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs.
                                <br>
                                <time datetime="2023-07-11">July 11, 2023</time>
                            </a>
                            <br>
                            <tag> Agent | Autonomous System </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.14824>Kosmos-2: Grounding Multimodal Large Language Models (MLLMs) to the World</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.14824 class="color-fg-50">
                               <tag>ICLR'24 </tag>We introduce KOSMOS-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. In addition to existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), KOSMOS-2 integrates the grounding capability into downstream applications.
                                <br>
                                <time datetime="2023-06-26">June 26, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-2 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.08543>Knowledge Distillation of Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.08543 class="color-fg-50">
                               <tag>ICLR'24 </tag>We propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution.
                                <br>
                                <time datetime="2023-06-14">June 14, 2023</time>
                            </a>
                            <br>
                            <tag> MiniLLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.07174>Augmenting Language Models with Long-Term Memory</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.07174 class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness.
                                <br>
                                <time datetime="2023-06-12">June 12, 2023</time>
                            </a>
                            <br>
                            <tag> #Long-term-Memory </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.10855>TextDiffuser: Diffusion Models as Text Painters</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.10855 class="color-fg-50">
                               <tag>NeurIPS'23 </tag>Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds.
                                <br>
                                <time datetime="2023-05-11">May 16, 2023</time>
                            </a>
                            <br>
                            <tag> #AIGC TextDiffuser </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.09137>Pre-Training to Learn in Context</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.09137 class="color-fg-50">
                               <tag>ACL'23 </tag>We propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of "intrinsic tasks" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models.
                                <br>
                                <time datetime="2023-05-11">May 16, 2023</time>
                            </a>
                            <br>
                            <tag> </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.07004>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.07004 class="color-fg-50">
                               <tag>EMNLP'23 </tag>We introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages.
                                <br>
                                <time datetime="2023-05-11">May 11, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps #Cross-Lingual-Thought </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2304.04487">Inference with Reference: Lossless Acceleration of Large Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2304.04487" class="color-fg-50">
                               We propose LLMA, an LLM accelerator to losslessly speed up Large Language Model (LLM) inference with references. LLMA is motivated by the observation that there are abundant identical text spans between the decoding result by an LLM and the reference that is available in many real world scenarios (e.g., retrieved documents). The improved computational parallelism allows LLMA to achieve over 2x speed-up for LLMs with identical generation results as greedy decoding in many practical generation scenarios.
                                <br>
                                <time datetime="2023-04-10">April 10, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps LLM Accelerator </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2304.03216">On the Pareto Front of Multilingual Neural Machine Translation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2304.03216" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses a great challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy, and the number of tasks.
                                <br>
                                <time datetime="2023-04-6">April 6, 2023</time>
                            </a>
                            <br>
                            <tag> #language #multilingual </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2302.14045">Language Is Not All You Need: Aligning Perception with Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2302.14045" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot).
                                <br>
                                <time datetime="2023-02-27">February 27, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-1 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2303.03926">Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2303.03926" class="color-fg-50">
                               We extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
                                <br>
                                <time datetime="2023-03-07">March 7, 2023</time>
                            </a>
                            <br>
                            <tag> #speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2301.02111">Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2301.02111" class="color-fg-50">
                               We train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt.
                                <br>
                                <time datetime="2023-01-06">January 6, 2023</time>
                            </a>
                            <br>
                            <tag> #speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.10554">A Length-Extrapolatable Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.10554" class="color-fg-50">
                               <tag>ACL'23 </tag>In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution.
                                <br>
                                <time datetime="2022-12-20">December 20, 2022</time>
                            </a>
                            <br>
                            <tag> TorchScale XPos </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.10559">Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.10559" class="color-fg-50">
                               <tag>ACL'23 </tag>This paper explains language models as meta optimizers and understands ICL as a kind of implicit finetuning. Theoretically, we figure out that the Transformer attention has a dual form of gradient descent based optimization. 
                                <br>
                                <time datetime="2022-12-20">December 20, 2022</time>
                            </a>
                            <br>
                            <tag> In-Context Learning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.09611">Optimizing Prompts for Text-to-Image Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.09611" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We propose prompt adaptation, a general framework based on reinforcement learning that automatically adapts original user input to model-preferred prompts. 
                                <br>
                                <time datetime="2022-12-19">December 19, 2022</time>
                            </a>
                            <br>
                            <tag> #LLMOps Promptist </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.09058">BEATs: Audio Pre-Training with Acoustic Tokenizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.09058" class="color-fg-50">
                               <tag>ICML'23 </tag>We propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. 
                                <br>
                                <time datetime="2022-12-18">December 18, 2022</time>
                            </a>
                            <br>
                            <tag> #speech </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.06713">Structured Prompting: Scaling In-Context Learning to 1,000 Examples</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.06713" class="color-fg-50">
                               Conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. 
                                <br>
                                <time datetime="2022-12-13">December 13, 2022</time>
                            </a>
                            <br>
                            <tag> #LLMOps Structured Prompting </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.00616">Extensible Prompts for Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.00616" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words that are introduced to help represent what NL words hardly describe, allowing a prompt to be more descriptive. 
                                <br>
                                <time datetime="2022-12-01">December 1, 2022</time>
                            </a>
                            <br>
                            <tag> #LLMOps X-Prompt </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.03533">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.03533" class="color-fg-50">
                               This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs).
                                <br>
                                <time datetime="2022-12-07">December 7, 2022</time>
                            </a>
                            <br>
                            <tag> E5 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2211.13184">TorchScale: Transformers at Scale</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2211.13184" class="color-fg-50">
                               We present TorchScale, an open-source toolkit that allows researchers and developers to scale up Transformers efficiently and effectively. TorchScale has the implementation of several modeling techniques, which can improve modeling generality and capability, as well as training stability and efficiency. 
                                <br>
                                <time datetime="2022-11-23">November 23, 2022</time>
                            </a>
                            <br>
                            <tag> TorchScale Magneto DeepNet X-MoE </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2210.06423">Magneto: A Foundation Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.10442" class="color-fg-50">
                                <tag>ICML'23 </tag>We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).
                                <br>
                                <time datetime="2022-10-12">October 12, 2022</time>
                            </a>
                            <br>
                            <tag> TorchScale Magneto DeepNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2208.10442">Image as a Foreign Language: BEiT Pretraining
                                for All Vision and Vision-Language Tasks</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.10442" class="color-fg-50">
                                <tag>CVPR'23 </tag>We present BEiT-3, a general-purpose multimodal foundation model which achieves SOTA across 10+ major vision and vision-language benchmarks. BEiT-3 = Multiway Transformers + Masked Data Modeling + Scaling Up.
                                <br>
                                <time datetime="2022-08-31">August 22, 2022</time>
                            </a>
                            <br>
                            <tag> #multimodal #vision BEiT-3 </tag> 
                        </div>
                    </div>
                </div>
                <!-- <div class="col-12 col-md-4">
                    <img class="position-absolute trbl-0 js-lazy js-lazy-loaded"
                                            alt="BEIT-3"
                                            src="./assets/img/beitv3.png"> -->
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2206.06336">Language Models are General-Purpose Interfaces</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2206.06336" class="color-fg-50">
                                We propose to use language models as a general-purpose interface to various foundation models. 
                                A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer.
                                <br>
                                <time datetime="2022-06-13">June 13, 2022</time>
                            </a>
                            <br>
                            <tag> #multimodal MetaLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2208.06366">BEiT v2: Masked Image Modeling with
                                Vector-Quantized Visual Tokenizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.06366" class="color-fg-50">
                                Use a semantic-rich visual tokenizer as the reconstruction target for masked prediction,
                                providing a systematic way to promote MIM from pixel-level to semantic-level.
                                <br>
                                <time datetime="2022-08-31">August 12, 2022</time>
                            </a>
                            <br>
                            <tag> #vision BEiT(-2) </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2210.10615">A Unified View of Masked Image Modeling</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2210.10615" class="color-fg-50">
                                We propose a unified view of masked image modeling after revisiting existing methods. Under the unified view, we introduce a simple yet effective method, 
                                termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images.
                                <br>
                                <time datetime="2022-10-19">October 19, 2022</time>
                            </a>
                            <br>
                            <tag> #vision MIM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2209.15329">SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2209.15329" class="color-fg-50">
                               We propose a cross-modal Speech and Language Model (SpeechLM) to explicitly align speech and text pre-training with a pre-defined unified discrete representation.
                                Leveraging only 10K text sentences, our SpeechLM gets a 16\% relative WER reduction over the best base model performance (from 6.8 to 5.7) on the public LibriSpeech ASR benchmark.
                                <br>
                                <time datetime="2022-09-30">September 30, 2022</time>
                            </a>
                            <br>
                            <tag> #speech SpeechLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2207.02578">SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2207.02578" class="color-fg-50">
                               <tag>ACL'23 </tag>We propose a simple yet effective pre-training method for dense passage retrieval.
                                <br>
                                <time datetime="2022-07-06">July 6, 2022</time>
                            </a>
                            <br>
                            <tag> #language SimLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2205.10350">Lossless Acceleration for Seq2seq Generation with Aggressive Decoding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2205.10350" class="color-fg-50">
                               We study lossless acceleration for seq2seq generation with a novel decoding algorithm -- Aggressive Decoding.
                                <br>
                                <time datetime="2022-05-20">May 20, 2022</time>
                            </a>
                            <br>
                            <tag> #language </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.09179">On the Representation Collapse of Sparse Mixture of Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.09179" class="color-fg-50">
                                 <tag>NeurIPS'22 </tag>Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.
                                <br>
                                <time datetime="2022-08-31">April 20, 2022</time>
                            </a>
                            <br>
                            <tag> #TorchScale #MoE X-MoE </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.08396">StableMoE: Stable Routing Strategy for Mixture of Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.08396" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose StableMoE with two training stages to address the routing fluctuation problem.
                                <br>
                                <time datetime="2022-08-31">April 18, 2022</time>
                            </a>
                            <br> <tag> #language #MoE </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.08387">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.08387" class="color-fg-50">
                                <tag>ACM MM'22 </tag>Pre-train multimodal Transformers for Document AI with unified text and image masking.
                                <br>
                                <time datetime="2022-08-31">April 18, 2022</time>
                            </a>
                            <br> <tag> #multimodal LayoutLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2203.02378">DiT: Self-supervised Pre-training for Document Image Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2203.02378" class="color-fg-50">
                                <tag> ACM MM'22 </tag>A self-supervised pre-trained DiT ransformer model using large-scale unlabeled text images for Document AI tasks.
                                <br>
                                <time datetime="2022-08-31">March 4, 2022</time>
                            </a>
                            <br><tag> #vision DiT BEiT </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2203.00555">DeepNet: Scaling Transformers to 1,000 Layers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2203.00555" class="color-fg-50">
                                <tag> TPAMI'24 </tag>We introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization.
                                <br>
                                <time datetime="2022-08-31">March 1, 2022</time>
                            </a>
                            <br>
                            <tag> DeepNet </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2202.07959">EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2202.07959" class="color-fg-50">
                                <tag>EMNLP'22</tag> We propose EdgeFormer -- a parameter-efficient Transformer of the encoder-decoder architecture for on-device seq2seq generation, which is customized under strict computation and memory constraints.
                                <br>
                                <time datetime="2022-02-16">February 16, 2022</time>
                            </a>
                            <br>
                            <tag> #language </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2111.02358">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2111.02358" class="color-fg-50">
                                <tag>NeurIPS'22</tag> We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network.. 
                                <br>
                                <time datetime="2021-11-03">November 03, 2021</time>
                            </a>
                            <br>
                            <tag> #multimodal VLMo Multiway Transformers </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.13900">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.13900" class="color-fg-50">
                                <tag>JSTSP'22</tag> Pre-training for full-stack speech processing tasks. WavLM jointly learns masked speech prediction and denoising in pre-training, and it achieves state-of-the-art performance on the SUPERB benchmark. 
                                <br>
                                <time datetime="2022-08-31">October 26, 2021</time>
                            </a>
                            <br>
                            <tag> #speech WavLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.08518">MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.08518" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. 
                                <br>
                                <time datetime="2021-10-16">October 16, 2021</time>
                            </a>
                            <br>
                            <tag> #language MarkupLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.07205">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.07205" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. 
                                <br>
                                <time datetime="2021-10-14">October 14, 2021</time>
                            </a>
                            <br>
                            <tag> #speech SpeechT5 </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2109.07306">Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2109.07306" class="color-fg-50">
                                <tag> EMNLP'21 </tag>We propose an algorithm VoCap to determine the desired vocabulary capacity of each language.
                                <br>
                                <time datetime="2022-09-15">September 15, 2021</time>
                            </a>
                            <br> <tag> #multilingual XLM-E </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.16138">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.16138" class="color-fg-50">
                                <tag> ACL'22 </tag>ELECTRA-style tasks to cross-lingual language model pre-training.
                                <br>
                                <time datetime="2022-08-31">June 30, 2021</time>
                            </a>
                            <br> <tag> #multilingual XLM-E </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.13474">Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.13474" class="color-fg-50">
                                <tag> ACL'21 </tag>We present a general approach to developing small, fast and effective pre-trained models for specific domains.
                                <br>
                                <time datetime="2021-06-25">June 25, 2021</time>
                            </a>
                            <br> <tag> #language AdaLM incr_bpe </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.08254">BEiT: BERT Pre-Training of Image Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.08254" class="color-fg-50">
                                <tag> ICLR'22 (<font color="blue">Oral</font>) </tag>We propose Maskded Image Modeling to pretrain vision Transformers.
                                <br>
                                <time datetime="2022-08-31">June 15, 2021</time>
                            </a> 
                            <br>
                            <tag> #vision BEiT </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.08226">Consistency Regularization for Cross-Lingual Fine-Tuning</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.08226" class="color-fg-50">
                                <tag> ACL'21 (<font color="blue">Oral</font>) </tag>We propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation.
                                <br>
                                <time datetime="2022-08-31">June 15, 2021</time>
                            </a> 
                            <br>
                            <tag> #multilingual xTune </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08836">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08836" class="color-fg-50">
                                <tag> ACL'22 </tag>We present a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding.
                                <br>
                                <time datetime="2022-08-31">April 18, 2021</time>
                            </a>
                            <br> <tag> #multimodal #multilingual Layout(X)LM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08692">mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08692" class="color-fg-50">
                                <tag> EMNLP'21 </tag>Improve multilingual text-to-text transfer Transformer with translation pairs.
                                <br>
                                <time datetime="2022-08-31">April 18, 2021</time>
                            </a>
                            <br> <tag> #multilingual DeltaLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08696">Knowledge Neurons in Pretrained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08696" class="color-fg-50">
                                <tag> ACL'22 </tag>A preliminary study on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons.
                                <br>
                                <time datetime="2022-08-31">March 10, 2021</time>
                            </a>
                            <br>
                            <tag> #language </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2101.07597">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2101.07597" class="color-fg-50">
                                <tag> ICML'21 </tag>we propose a unified pre-training approach to learning speech representations with both unlabeled and labeled data.
                                <br>
                                <time datetime="2021-01-19">January 19, 2021</time>
                            </a>
                            <br>
                            <tag> #speech </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2012.15828">MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2012.15828" class="color-fg-50">
                                <tag> ACL'21 </tag>Employ multi-head self-attention relations (scaled dot-product between the pairs of query, key, and value vectors within each self-attention module) to train the student model.
                                <br>
                                <time datetime="2022-08-31">December 31, 2020</time>
                            </a>
                            <br><tag> #language MiniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
          <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2012.14740">LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2012.14740" class="color-fg-50">
                                <tag> ACL'21 </tag>We propose a new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework.
                                <br>
                                <time datetime="2020-12-29">December 29, 2020</time>
                            </a>
                            <br><tag> #multimodal LayoutLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2007.07834">InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2007.07834" class="color-fg-50">
                                <tag> NAACL'21 </tag>An information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. 
                                <br>
                                <time datetime="2022-08-31">July 15, 2020</time>
                            </a>
                            <br><tag> #multilingual InfoXLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2004.11207">Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2004.11207" class="color-fg-50">
                                <tag> AAAI'21 (<font color="blue">Best Paper Runner Up</font>) </tag>A self-attention attribution method to interpret the information interactions inside Transformer..
                                <br>
                                <time datetime="2022-08-31">April 23, 2020</time>
                            </a>
                            <br><tag> #language </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2002.12804">UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08692" class="color-fg-50">
                                <tag> ICLM'20 </tag>Pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model. 
                                <br>
                                <time datetime="2022-08-31">February 28, 2020</time></a>
                            <br>
                            <tag> #language UniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2002.10957">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2002.10957" class="color-fg-50">
                                <tag> NeurIPS'20 </tag>A simple and effective approach to compress large Transformer based pre-trained models, termed as deep self-attention distillation.
                                <br>
                                <time datetime="2022-08-31">February 25, 2020</time>
                            </a>
                            <br> <tag> #language MiniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>     
        

        <div class="post-card-full medium-xsmall-copy" data-year="2019">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2019</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/1912.13318">LayoutLM: Pre-training of Text and Layout for Document Image Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/1912.13318" class="color-fg-50">
                                <tag> KDD'20 </tag>Jointly model interactions between text and layout information across scanned document images.
                                <br>
                                <time datetime="2022-08-31">December 31, 2019</time>
                            </a>
                            <br> <tag> #multimodal LayoutLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2019">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/1905.03197">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/1905.03197" class="color-fg-50">
                                <tag> NeurIPS'19 </tag>A new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. 
                                <br>
                                <time datetime="2022-08-31">May 8, 2019</time>
                            </a>
                            <br><tag> #language UniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <footer class="footer container medium-xsmall-copy line-height-1.6">

        <div class="row align-items-center mb-0.125">
            <div class="col-12 col-md mb-0.5">
                <a class="fade" style="margin-top:1px" href="/">&copy; 2022</a>
            </div>
        </div>
    </footer>

    <script type="text/javascript" src="./assets/js/main.min.js"></script>


</body>

</html>
