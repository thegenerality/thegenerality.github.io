<!DOCTYPE html>
<html lang="en">

<head>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-71156606-1');
    </script>
    <meta charset="utf-8" />
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Advancing AI for humanity | Foundation of AI</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="./assets/css/main.css" />

    <script type="text/javascript">document.documentElement.className = 'js';</script>
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:image:width" content="3200" />
    <meta property="og:image:height" content="1800" />

    <meta name="generator" content="Ghost 5.12" />
    <style id="gh-members-styles">
        .gh-post-upgrade-cta-content,
        .gh-post-upgrade-cta {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            text-align: center;
            width: 100%;
            color: #ffffff;
            font-size: 16px;
        }

        .gh-post-upgrade-cta-content {
            border-radius: 8px;
            padding: 40px 4vw;
        }

        .gh-post-upgrade-cta h2 {
            color: #ffffff;
            font-size: 28px;
            letter-spacing: -0.2px;
            margin: 0;
            padding: 0;
        }

        .gh-post-upgrade-cta p {
            margin: 20px 0 0;
            padding: 0;
        }

        .gh-post-upgrade-cta small {
            font-size: 16px;
            letter-spacing: -0.2px;
        }

        .gh-post-upgrade-cta a {
            color: #ffffff;
            cursor: pointer;
            font-weight: 500;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a:hover {
            color: #ffffff;
            opacity: 0.8;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a.gh-btn {
            display: block;
            background: #ffffff;
            text-decoration: none;
            margin: 28px 0 0;
            padding: 8px 18px;
            border-radius: 4px;
            font-size: 16px;
            font-weight: 600;
        }

        .gh-post-upgrade-cta a.gh-btn:hover {
            opacity: 0.92;
        }
    </style>
    <script defer src="./assets/js/cards.min.js"></script>
    <style>
        :root {
            --ghost-accent-color: #15171A;
        }
        tag {
            /* color: -webkit-link; */
            font-style: italic;
            font-size: 16px;
            color: black;
            /* color: white;
            background-color: #264653!important;
            font-weight: 600;
            padding: 0px 3px 0 3px;
            margin-right: 5px; */
        }
        /* tag.conf {
            color: white;
            background-color: #2a9d8f!important;
            font-weight: 600;
            padding: 0px 3px 0 3px;
        } */
        
    </style>
    <link rel="stylesheet" type="text/css" href="./assets/css/cards.min.css">
</head>

<body>

    <header>
        <nav class="nav container" data-url="/blog/">
            <div class="nav-row row align-items-center">
                <div class="d-none d-sm-block col-sm nav-symbol-wrap">
                    <a href="./index.html"> Home </a>
                </div>
                <div class="col col-sm-auto">
                    <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
                        <div class="d-sm-none nav-symbol-wrap">
                        </div>
                        
                        <!--li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="overview.html"
                                data-slug="research">Overview</a></li-->

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link active" href="research.html"
                                data-slug="research">Research</a></li>

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="blog.html"
                                data-slug="blog">Blog</a></li>

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="about.html"
                                data-slug="about">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>

    </header>
    
   
    <div class="container mt-4">
        <h2 class="mb-2">Publication</h2>
        <p style="padding-bottom:20px"><a href="overview.html"><font color="blue">Highlights</font></a></p>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2024</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2405.05254>You Only Cache Once: Decoder-Decoder Architectures for Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2405.05254 class="color-fg-50">
                               We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes.
                               <br>
                                <time datetime="2024-05-09">May 9, 2024</time>
                            </a>
                            <br>
                            <tag> YOCO | RetNet(-3) </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf>The Era of 1-bit LLMs: Training Tips, Code and FAQ</a></h5>
                        <div>
                            <a href=https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf class="color-fg-50">
                               We present details and tips for training 1-bit LLMs. We also provide additional experiments and results that were not reported and responses to questions regarding the ``The-Era-of-1-bit-LLM'' paper. Finally, we include the official PyTorch implementation of BitNet (b1.58 and b1) for future research and development of 1-bit LLMs.
                                <br>
                                <time datetime="2024-03-20">Mar 20, 2024</time>
                            </a>
                            <br>
                            <tag> BitNet | The Era of 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.17764>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.17764 class="color-fg-50">
                               Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.
                                <br>
                                <time datetime="2024-02-28">Feb 28, 2024</time>
                            </a>
                            <br>
                            <tag> BitNet | The Era of 1-bit LLMs </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2406.14491>Instruction Pre-Training: Language Models are Supervised Multitask Learners</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2406.14491 class="color-fg-50">
                               Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning.
                                <br>
                                <time datetime="2024-06-20">Jun 20, 2024</time>
                            </a>
                            <br>
                            <tag> LLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2406.05370>VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2406.05370 class="color-fg-50">
                               This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis.
                                <br>
                                <time datetime="2024-06-08">Jun 8, 2024</time>
                            </a>
                            <br>
                            <tag> VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.15045>Multi-Head Mixture-of-Experts</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.15045 class="color-fg-50">
                               We propose Multi-Head Mixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each token into multiple sub-tokens. These sub-tokens are then assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form. The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting. Extensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE.

                                <br>
                                <time datetime="2024-04-23">Apr 23, 2024</time>
                            </a>
                            <br>
                            <tag> MoE </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2405.12130>MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2405.12130 class="color-fg-50">
                               Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.

                                <br>
                                <time datetime="2024-05-20">May 20, 2024</time>
                            </a>
                            <br>
                            <tag> MoRA </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.17759>Towards Optimal Learning of Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.17759 class="color-fg-50">
                               This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an "LM-training-as-lossless-compression" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods.
                                <br>
                                <time datetime="2024-02-28">Feb 28, 2024</time>
                            </a>
                            <br>
                            <tag> Learning Law </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.03622>Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.03622 class="color-fg-50">
                               We propose Visualization-of-Thought (VoT) prompting to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate \textit{mental images} to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.
                                <br>
                                <time datetime="2024-04-04">Apr 4, 2024</time>
                            </a>
                            <br>
                            <tag> Spatial Reasoning | VoT</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.01230>LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.01230 class="color-fg-50">
                               This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly.
                                <br>
                                <time datetime="2024-04-04">Apr 4, 2024</time>
                            </a>
                            <br>
                            <tag> Strategic Reasoning | Agent</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2404.00656>WavLLM: Towards Robust and Adaptive Speech Large Language Model</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2404.00656 class="color-fg-50">
                               We introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. 
                                <br>
                                <time datetime="2024-03-31">Mar 31, 2024</time>
                            </a>
                            <br>
                            <tag> MLLM | Speech LLMs</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2403.02884>MathScale: Scaling Instruction Tuning for Mathematical Reasoning</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2403.02884 class="color-fg-50">
                               We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs. Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate.
                                <br>
                                <time datetime="2024-03-05">Mar 5, 2024</time>
                            </a>
                            <br>
                            <tag> Scaling | Instruction Tuning</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.18039>ResLoRA: Identity Residual Mapping in Low-Rank Adaption</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.18039 class="color-fg-50">
                               Updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA.
                                <br>
                                <time datetime="2024-02-28">Feb 28, 2024</time>
                            </a>
                            <br>
                            <tag> LLM</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.16438>Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.16438 class="color-fg-50">
                               In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs.
                                <br>
                                <time datetime="2024-02-26">Feb 26, 2024</time>
                            </a>
                            <br>
                            <tag> LLM</tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.13064>Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.13064 class="color-fg-50">
                               We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.
                                <br>
                                <time datetime="2024-02-20">Feb 20, 2024</time>
                            </a>
                            <br>
                            <tag> GLAN </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2402.05672>Multilingual E5 Text Embeddings: A Technical Report</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2402.05672 class="color-fg-50">
                               This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes.
                                <br>
                                <time datetime="2024-02-02">Feb 2, 2024</time>
                            </a>
                            <br>
                            <tag> E5 embeddings </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2024">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://browse.arxiv.org/abs/2402.01521>K-Level Reasoning with Large Language Models</a></h5>
                        <div>
                            <a href=https://browse.arxiv.org/abs/2402.01521 class="color-fg-50">
                               We propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on available historical information, which significantly improves the prediction accuracy of rivals' subsequent moves and informs more strategic decision-making. This research not only sets a robust quantitative benchmark for the assessment of dynamic reasoning but also markedly enhances the proficiency of LLMs in dynamic contexts.
                                <br>
                                <time datetime="2024-02-02">Feb 2, 2024</time>
                            </a>
                            <br>
                            <tag> Agent Reasoning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2401.00368>Improving Text Embeddings with Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2401.00368 class="color-fg-50">
                               <tag>ACL'24 </tag>In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.
                                <br>
                                <time datetime="2023-12-31">Dec 31, 2023</time>
                            </a>
                            <br>
                            <tag> E5 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2312.03558>When an Image is Worth 1,024 × 1,024 Words: A Case Study in Computational Pathology</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2312.03558 class="color-fg-50">
                               We present LongViT, a vision Transformer that can process gigapixel images in an end-to-end manner. Specifically, we split the gigapixel image into a sequence of millions of patches and project them linearly into embeddings. LongNet is then employed to model the extremely long sequence, generating representations that capture both short-range and long-range dependencies. The linear computation complexity of LONGNET, along with its distributed algorithm, enables us to overcome the constraints of both computation and memory. 
                                <br>
                                <time datetime="2023-12-07">Dec 7, 2023</time>
                            </a>
                            <br>
                            <tag> LongViT LongNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2311.16465>TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2311.16465 class="color-fg-50">
                               We present TextDiffuser-2, aiming to unleash the power of language models for text rendering. Firstly, we fine-tune a large language model for layout planning. The large language model is capable of automatically generating keywords for text rendering and also supports layout modification through chatting. Secondly, we utilize the language model within the diffusion model to encode the position and texts at the line level. Unlike previous methods that employed tight character-level guidance, this approach generates more diverse text images. 
                                <br>
                                <time datetime="2023-11-29">Nov 29, 2023</time>
                            </a>
                            <br>
                            <tag> TextDiffuser </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2311.03220>ALYMPICS: Language Agents Meet Game Theory</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2311.03220 class="color-fg-50">
                               This paper introduces Alympics, a platform that leverages Large Language Model (LLM) agents to facilitate investigations in game theory. By employing LLMs and autonomous agents to simulate human behavior and enable multi-agent collaborations, we can construct realistic and dynamic models of human interactions for game theory hypothesis formulating and testing.
                                <br>
                                <time datetime="2023-11-06">Nov 6, 2023</time>
                            </a>
                            <br>
                            <tag> #Agent ALYMPICS </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2310.11453>BitNet: Scaling 1-bit Transformers for Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2310.11453 class="color-fg-50">
                               We introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.
                                <br>
                                <time datetime="2023-10-18">Oct 18, 2023</time>
                            </a>
                            <br>
                            <tag> BitNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2310.13385>Tuna: Instruction Tuning using Feedback from Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2310.13385 class="color-fg-50">
                               <tag>EMNLP'23 </tag>We propose finetuning an instruction-tuned LLM using our novel probabilistic ranking and contextual ranking approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs.
                                <br>
                                <time datetime="2023-10-20">Oct 20, 2023</time>
                            </a>
                            <br>
                            <tag> #Instruction_Tuning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2310.02992>Kosmos-G: Generating Images in Context with Multimodal Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2310.02992 class="color-fg-50">
                                <tag>ICLR'24 </tag>This paper presents KOSMOS-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. KOSMOS-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit KOSMOS-G as an initial attempt towards the goal of “image as a foreign language in image generation.”
                                <br>
                                <time datetime="2023-09-20">Sep 20, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-G </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.11419>Kosmos-2.5: A Multimodal Literate Model</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.11419 class="color-fg-50">
                                We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. 
                                <br>
                                <time datetime="2023-09-20">Sep 20, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-2.5 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.05689>Large Language Model for Science: A Study on P vs. NP</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.05689 class="color-fg-50">
                                We propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding "P ≠ NP". The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.
                                <br>
                                <time datetime="2023-09-13">Sep 13, 2023</time>
                            </a>
                            <br>
                            <tag> LLM4S </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        

        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.08621>Retentive Network: A Successor to Transformer for Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.08621 class="color-fg-50">
                               We propose retentive network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. The intriguing properties make
RetNet a strong successor to Transformer for large language models.
                                <br>
                                <time datetime="2023-07-18">July 18, 2023</time>
                            </a>
                            <br>
                            <tag> RetNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.02486>LongNet: Scaling Transformers to 1,000,000,000 Tokens</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.02486 class="color-fg-50">
                               We introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. 
                                <br>
                                <time datetime="2023-07-06">July 6, 2023</time>
                            </a>
                            <br>
                            <tag> LongNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.10400>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.10400 class="color-fg-50">
                               <tag>ICLR'24 </tag>We introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. 
                                <br>
                                <time datetime="2023-09-19">September 19, 2023</time>
                            </a>
                            <br>
                            <tag> #Long-context-modeling </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://openreview.net/forum?id=uWvKBCYh4S>MoLE: Mixture of LoRA Experts</a></h5>
                        <div>
                            <a href=https://openreview.net/forum?id=uWvKBCYh4S class="color-fg-50">
                               <tag>ICLR'24 </tag>This paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision & Language (V&L) domains substantiate the efficacy of MoLE. 
                                <br>
                                <time datetime="2023-09-18">September 18, 2023</time>
                            </a>
                            <br>
                            <tag>  </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2309.09530>Adapting Large Language Models via Reading Comprehension</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2309.09530 class="color-fg-50">
                               <tag>ICLR'24 </tag>We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. 
                                <br>
                                <time datetime="2023-09-18">September 18, 2023</time>
                            </a>
                            <br>
                            <tag>  </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2308.12770>WavMark: Watermarking for Audio Generation</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2308.12770 class="color-fg-50">
                               This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. 
                                <br>
                                <time datetime="2023-08-24">August 24, 2023</time>
                            </a>
                            <br>
                            <tag> #speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
                
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.07164>Learning to Retrieve In-Context Examples for Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.07164 class="color-fg-50">
                               <tag>EACL'24 </tag>We propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. 
                                <br>
                                <time datetime="2023-07-14">July 14, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.06945>In-context Autoencoder for Context Compression in a Large Language Model</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.06945 class="color-fg-50">
                               <tag>ICLR'24 </tag>We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with 4× context compression, which can be well conditioned on by the target LLM to respond to various prompts. 
                                <br>
                                <time datetime="2023-07-13">July 13, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2307.05300>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2307.05300 class="color-fg-50">
                              We propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs.
                                <br>
                                <time datetime="2023-07-11">July 11, 2023</time>
                            </a>
                            <br>
                            <tag> Agent | Autonomous System </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.14824>Kosmos-2: Grounding Multimodal Large Language Models (MLLMs) to the World</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.14824 class="color-fg-50">
                               <tag>ICLR'24 </tag>We introduce KOSMOS-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. In addition to existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), KOSMOS-2 integrates the grounding capability into downstream applications.
                                <br>
                                <time datetime="2023-06-26">June 26, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-2 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.08543>Knowledge Distillation of Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.08543 class="color-fg-50">
                               <tag>ICLR'24 </tag>We propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution.
                                <br>
                                <time datetime="2023-06-14">June 14, 2023</time>
                            </a>
                            <br>
                            <tag> MiniLLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.07174>Augmenting Language Models with Long-Term Memory</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.07174 class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness.
                                <br>
                                <time datetime="2023-06-12">June 12, 2023</time>
                            </a>
                            <br>
                            <tag> #Long-term-Memory </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.10855>TextDiffuser: Diffusion Models as Text Painters</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.10855 class="color-fg-50">
                               <tag>NeurIPS'23 </tag>Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds.
                                <br>
                                <time datetime="2023-05-11">May 16, 2023</time>
                            </a>
                            <br>
                            <tag> #AIGC TextDiffuser </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.09137>Pre-Training to Learn in Context</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.09137 class="color-fg-50">
                               <tag>ACL'23 </tag>We propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of "intrinsic tasks" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models.
                                <br>
                                <time datetime="2023-05-11">May 16, 2023</time>
                            </a>
                            <br>
                            <tag> </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.07004>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.07004 class="color-fg-50">
                               <tag>EMNLP'23 </tag>We introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages.
                                <br>
                                <time datetime="2023-05-11">May 11, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps #Cross-Lingual-Thought </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2304.04487">Inference with Reference: Lossless Acceleration of Large Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2304.04487" class="color-fg-50">
                               We propose LLMA, an LLM accelerator to losslessly speed up Large Language Model (LLM) inference with references. LLMA is motivated by the observation that there are abundant identical text spans between the decoding result by an LLM and the reference that is available in many real world scenarios (e.g., retrieved documents). The improved computational parallelism allows LLMA to achieve over 2x speed-up for LLMs with identical generation results as greedy decoding in many practical generation scenarios.
                                <br>
                                <time datetime="2023-04-10">April 10, 2023</time>
                            </a>
                            <br>
                            <tag> #LLMOps LLM Accelerator </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2304.03216">On the Pareto Front of Multilingual Neural Machine Translation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2304.03216" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses a great challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy, and the number of tasks.
                                <br>
                                <time datetime="2023-04-6">April 6, 2023</time>
                            </a>
                            <br>
                            <tag> #language #multilingual </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2302.14045">Language Is Not All You Need: Aligning Perception with Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2302.14045" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot).
                                <br>
                                <time datetime="2023-02-27">February 27, 2023</time>
                            </a>
                            <br>
                            <tag> #MLLM Kosmos-1 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2303.03926">Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2303.03926" class="color-fg-50">
                               We extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
                                <br>
                                <time datetime="2023-03-07">March 7, 2023</time>
                            </a>
                            <br>
                            <tag> #speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2301.02111">Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2301.02111" class="color-fg-50">
                               We train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt.
                                <br>
                                <time datetime="2023-01-06">January 6, 2023</time>
                            </a>
                            <br>
                            <tag> #speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.10554">A Length-Extrapolatable Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.10554" class="color-fg-50">
                               <tag>ACL'23 </tag>In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution.
                                <br>
                                <time datetime="2022-12-20">December 20, 2022</time>
                            </a>
                            <br>
                            <tag> TorchScale XPos </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.10559">Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.10559" class="color-fg-50">
                               <tag>ACL'23 </tag>This paper explains language models as meta optimizers and understands ICL as a kind of implicit finetuning. Theoretically, we figure out that the Transformer attention has a dual form of gradient descent based optimization. 
                                <br>
                                <time datetime="2022-12-20">December 20, 2022</time>
                            </a>
                            <br>
                            <tag> In-Context Learning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.09611">Optimizing Prompts for Text-to-Image Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.09611" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We propose prompt adaptation, a general framework based on reinforcement learning that automatically adapts original user input to model-preferred prompts. 
                                <br>
                                <time datetime="2022-12-19">December 19, 2022</time>
                            </a>
                            <br>
                            <tag> #LLMOps Promptist </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.09058">BEATs: Audio Pre-Training with Acoustic Tokenizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.09058" class="color-fg-50">
                               <tag>ICML'23 </tag>We propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. 
                                <br>
                                <time datetime="2022-12-18">December 18, 2022</time>
                            </a>
                            <br>
                            <tag> #speech </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.06713">Structured Prompting: Scaling In-Context Learning to 1,000 Examples</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.06713" class="color-fg-50">
                               Conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. 
                                <br>
                                <time datetime="2022-12-13">December 13, 2022</time>
                            </a>
                            <br>
                            <tag> #LLMOps Structured Prompting </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.00616">Extensible Prompts for Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.00616" class="color-fg-50">
                               <tag>NeurIPS'23 </tag>We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words that are introduced to help represent what NL words hardly describe, allowing a prompt to be more descriptive. 
                                <br>
                                <time datetime="2022-12-01">December 1, 2022</time>
                            </a>
                            <br>
                            <tag> #LLMOps X-Prompt </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.03533">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.03533" class="color-fg-50">
                               This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs).
                                <br>
                                <time datetime="2022-12-07">December 7, 2022</time>
                            </a>
                            <br>
                            <tag> E5 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2211.13184">TorchScale: Transformers at Scale</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2211.13184" class="color-fg-50">
                               We present TorchScale, an open-source toolkit that allows researchers and developers to scale up Transformers efficiently and effectively. TorchScale has the implementation of several modeling techniques, which can improve modeling generality and capability, as well as training stability and efficiency. 
                                <br>
                                <time datetime="2022-11-23">November 23, 2022</time>
                            </a>
                            <br>
                            <tag> TorchScale Magneto DeepNet X-MoE </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2210.06423">Magneto: A Foundation Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.10442" class="color-fg-50">
                                <tag>ICML'23 </tag>We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).
                                <br>
                                <time datetime="2022-10-12">October 12, 2022</time>
                            </a>
                            <br>
                            <tag> TorchScale Magneto DeepNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2208.10442">Image as a Foreign Language: BEiT Pretraining
                                for All Vision and Vision-Language Tasks</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.10442" class="color-fg-50">
                                <tag>CVPR'23 </tag>We present BEiT-3, a general-purpose multimodal foundation model which achieves SOTA across 10+ major vision and vision-language benchmarks. BEiT-3 = Multiway Transformers + Masked Data Modeling + Scaling Up.
                                <br>
                                <time datetime="2022-08-31">August 22, 2022</time>
                            </a>
                            <br>
                            <tag> #multimodal #vision BEiT-3 </tag> 
                        </div>
                    </div>
                </div>
                <!-- <div class="col-12 col-md-4">
                    <img class="position-absolute trbl-0 js-lazy js-lazy-loaded"
                                            alt="BEIT-3"
                                            src="./assets/img/beitv3.png"> -->
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2206.06336">Language Models are General-Purpose Interfaces</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2206.06336" class="color-fg-50">
                                We propose to use language models as a general-purpose interface to various foundation models. 
                                A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer.
                                <br>
                                <time datetime="2022-06-13">June 13, 2022</time>
                            </a>
                            <br>
                            <tag> #multimodal MetaLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2208.06366">BEiT v2: Masked Image Modeling with
                                Vector-Quantized Visual Tokenizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.06366" class="color-fg-50">
                                Use a semantic-rich visual tokenizer as the reconstruction target for masked prediction,
                                providing a systematic way to promote MIM from pixel-level to semantic-level.
                                <br>
                                <time datetime="2022-08-31">August 12, 2022</time>
                            </a>
                            <br>
                            <tag> #vision BEiT(-2) </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2210.10615">A Unified View of Masked Image Modeling</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2210.10615" class="color-fg-50">
                                We propose a unified view of masked image modeling after revisiting existing methods. Under the unified view, we introduce a simple yet effective method, 
                                termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images.
                                <br>
                                <time datetime="2022-10-19">October 19, 2022</time>
                            </a>
                            <br>
                            <tag> #vision MIM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2209.15329">SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2209.15329" class="color-fg-50">
                               We propose a cross-modal Speech and Language Model (SpeechLM) to explicitly align speech and text pre-training with a pre-defined unified discrete representation.
                                Leveraging only 10K text sentences, our SpeechLM gets a 16\% relative WER reduction over the best base model performance (from 6.8 to 5.7) on the public LibriSpeech ASR benchmark.
                                <br>
                                <time datetime="2022-09-30">September 30, 2022</time>
                            </a>
                            <br>
                            <tag> #speech SpeechLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2207.02578">SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2207.02578" class="color-fg-50">
                               <tag>ACL'23 </tag>We propose a simple yet effective pre-training method for dense passage retrieval.
                                <br>
                                <time datetime="2022-07-06">July 6, 2022</time>
                            </a>
                            <br>
                            <tag> #language SimLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2205.10350">Lossless Acceleration for Seq2seq Generation with Aggressive Decoding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2205.10350" class="color-fg-50">
                               We study lossless acceleration for seq2seq generation with a novel decoding algorithm -- Aggressive Decoding.
                                <br>
                                <time datetime="2022-05-20">May 20, 2022</time>
                            </a>
                            <br>
                            <tag> #language </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.09179">On the Representation Collapse of Sparse Mixture of Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.09179" class="color-fg-50">
                                 <tag>NeurIPS'22 </tag>Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.
                                <br>
                                <time datetime="2022-08-31">April 20, 2022</time>
                            </a>
                            <br>
                            <tag> #TorchScale #MoE X-MoE </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.08396">StableMoE: Stable Routing Strategy for Mixture of Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.08396" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose StableMoE with two training stages to address the routing fluctuation problem.
                                <br>
                                <time datetime="2022-08-31">April 18, 2022</time>
                            </a>
                            <br> <tag> #language #MoE </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.08387">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.08387" class="color-fg-50">
                                <tag>ACM MM'22 </tag>Pre-train multimodal Transformers for Document AI with unified text and image masking.
                                <br>
                                <time datetime="2022-08-31">April 18, 2022</time>
                            </a>
                            <br> <tag> #multimodal LayoutLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2203.02378">DiT: Self-supervised Pre-training for Document Image Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2203.02378" class="color-fg-50">
                                <tag> ACM MM'22 </tag>A self-supervised pre-trained DiT ransformer model using large-scale unlabeled text images for Document AI tasks.
                                <br>
                                <time datetime="2022-08-31">March 4, 2022</time>
                            </a>
                            <br><tag> #vision DiT BEiT </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2203.00555">DeepNet: Scaling Transformers to 1,000 Layers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2203.00555" class="color-fg-50">
                                <tag> TPAMI'24 </tag>We introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization.
                                <br>
                                <time datetime="2022-08-31">March 1, 2022</time>
                            </a>
                            <br>
                            <tag> DeepNet </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2202.07959">EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2202.07959" class="color-fg-50">
                                <tag>EMNLP'22</tag> We propose EdgeFormer -- a parameter-efficient Transformer of the encoder-decoder architecture for on-device seq2seq generation, which is customized under strict computation and memory constraints.
                                <br>
                                <time datetime="2022-02-16">February 16, 2022</time>
                            </a>
                            <br>
                            <tag> #language </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2111.02358">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2111.02358" class="color-fg-50">
                                <tag>NeurIPS'22</tag> We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network.. 
                                <br>
                                <time datetime="2021-11-03">November 03, 2021</time>
                            </a>
                            <br>
                            <tag> #multimodal VLMo Multiway Transformers </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.13900">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.13900" class="color-fg-50">
                                <tag>JSTSP'22</tag> Pre-training for full-stack speech processing tasks. WavLM jointly learns masked speech prediction and denoising in pre-training, and it achieves state-of-the-art performance on the SUPERB benchmark. 
                                <br>
                                <time datetime="2022-08-31">October 26, 2021</time>
                            </a>
                            <br>
                            <tag> #speech WavLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.08518">MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.08518" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. 
                                <br>
                                <time datetime="2021-10-16">October 16, 2021</time>
                            </a>
                            <br>
                            <tag> #language MarkupLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.07205">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.07205" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. 
                                <br>
                                <time datetime="2021-10-14">October 14, 2021</time>
                            </a>
                            <br>
                            <tag> #speech SpeechT5 </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2109.07306">Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2109.07306" class="color-fg-50">
                                <tag> EMNLP'21 </tag>We propose an algorithm VoCap to determine the desired vocabulary capacity of each language.
                                <br>
                                <time datetime="2022-09-15">September 15, 2021</time>
                            </a>
                            <br> <tag> #multilingual XLM-E </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.16138">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.16138" class="color-fg-50">
                                <tag> ACL'22 </tag>ELECTRA-style tasks to cross-lingual language model pre-training.
                                <br>
                                <time datetime="2022-08-31">June 30, 2021</time>
                            </a>
                            <br> <tag> #multilingual XLM-E </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.13474">Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.13474" class="color-fg-50">
                                <tag> ACL'21 </tag>We present a general approach to developing small, fast and effective pre-trained models for specific domains.
                                <br>
                                <time datetime="2021-06-25">June 25, 2021</time>
                            </a>
                            <br> <tag> #language AdaLM incr_bpe </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.08254">BEiT: BERT Pre-Training of Image Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.08254" class="color-fg-50">
                                <tag> ICLR'22 (<font color="blue">Oral</font>) </tag>We propose Maskded Image Modeling to pretrain vision Transformers.
                                <br>
                                <time datetime="2022-08-31">June 15, 2021</time>
                            </a> 
                            <br>
                            <tag> #vision BEiT </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.08226">Consistency Regularization for Cross-Lingual Fine-Tuning</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.08226" class="color-fg-50">
                                <tag> ACL'21 (<font color="blue">Oral</font>) </tag>We propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation.
                                <br>
                                <time datetime="2022-08-31">June 15, 2021</time>
                            </a> 
                            <br>
                            <tag> #multilingual xTune </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08836">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08836" class="color-fg-50">
                                <tag> ACL'22 </tag>We present a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding.
                                <br>
                                <time datetime="2022-08-31">April 18, 2021</time>
                            </a>
                            <br> <tag> #multimodal #multilingual Layout(X)LM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08692">mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08692" class="color-fg-50">
                                <tag> EMNLP'21 </tag>Improve multilingual text-to-text transfer Transformer with translation pairs.
                                <br>
                                <time datetime="2022-08-31">April 18, 2021</time>
                            </a>
                            <br> <tag> #multilingual DeltaLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08696">Knowledge Neurons in Pretrained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08696" class="color-fg-50">
                                <tag> ACL'22 </tag>A preliminary study on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons.
                                <br>
                                <time datetime="2022-08-31">March 10, 2021</time>
                            </a>
                            <br>
                            <tag> #language </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2101.07597">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2101.07597" class="color-fg-50">
                                <tag> ICML'21 </tag>we propose a unified pre-training approach to learning speech representations with both unlabeled and labeled data.
                                <br>
                                <time datetime="2021-01-19">January 19, 2021</time>
                            </a>
                            <br>
                            <tag> #speech </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2012.15828">MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2012.15828" class="color-fg-50">
                                <tag> ACL'21 </tag>Employ multi-head self-attention relations (scaled dot-product between the pairs of query, key, and value vectors within each self-attention module) to train the student model.
                                <br>
                                <time datetime="2022-08-31">December 31, 2020</time>
                            </a>
                            <br><tag> #language MiniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
          <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2012.14740">LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2012.14740" class="color-fg-50">
                                <tag> ACL'21 </tag>We propose a new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework.
                                <br>
                                <time datetime="2020-12-29">December 29, 2020</time>
                            </a>
                            <br><tag> #multimodal LayoutLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2007.07834">InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2007.07834" class="color-fg-50">
                                <tag> NAACL'21 </tag>An information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. 
                                <br>
                                <time datetime="2022-08-31">July 15, 2020</time>
                            </a>
                            <br><tag> #multilingual InfoXLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2004.11207">Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2004.11207" class="color-fg-50">
                                <tag> AAAI'21 (<font color="blue">Best Paper Runner Up</font>) </tag>A self-attention attribution method to interpret the information interactions inside Transformer..
                                <br>
                                <time datetime="2022-08-31">April 23, 2020</time>
                            </a>
                            <br><tag> #language </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2002.12804">UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08692" class="color-fg-50">
                                <tag> ICLM'20 </tag>Pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model. 
                                <br>
                                <time datetime="2022-08-31">February 28, 2020</time></a>
                            <br>
                            <tag> #language UniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2002.10957">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2002.10957" class="color-fg-50">
                                <tag> NeurIPS'20 </tag>A simple and effective approach to compress large Transformer based pre-trained models, termed as deep self-attention distillation.
                                <br>
                                <time datetime="2022-08-31">February 25, 2020</time>
                            </a>
                            <br> <tag> #language MiniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>     
        

        <div class="post-card-full medium-xsmall-copy" data-year="2019">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2019</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/1912.13318">LayoutLM: Pre-training of Text and Layout for Document Image Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/1912.13318" class="color-fg-50">
                                <tag> KDD'20 </tag>Jointly model interactions between text and layout information across scanned document images.
                                <br>
                                <time datetime="2022-08-31">December 31, 2019</time>
                            </a>
                            <br> <tag> #multimodal LayoutLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2019">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/1905.03197">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/1905.03197" class="color-fg-50">
                                <tag> NeurIPS'19 </tag>A new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. 
                                <br>
                                <time datetime="2022-08-31">May 8, 2019</time>
                            </a>
                            <br><tag> #language UniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <footer class="footer container medium-xsmall-copy line-height-1.6">

        <div class="row align-items-center mb-0.125">
            <div class="col-12 col-md mb-0.5">
                <a class="fade" style="margin-top:1px" href="/">&copy; 2022</a>
            </div>
        </div>
    </footer>

    <script type="text/javascript" src="./assets/js/main.min.js"></script>


</body>

</html>
